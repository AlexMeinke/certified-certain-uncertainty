{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils.models as models\n",
    "import utils.plotting as plotting\n",
    "import utils.dataloaders as dl\n",
    "import utils.traintest as tt\n",
    "import utils.adversarial as adv\n",
    "import utils.eval as ev\n",
    "import model_params as params\n",
    "import utils.resnet_orig as resnet\n",
    "import utils.gmm_helpers as gmm_helpers\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:5')\n",
    "\n",
    "dataset = 'FMNIST'\n",
    "\n",
    "model_params = params.params_dict[dataset](augm_flag=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "file = 'base_FMNIST_lr0.1_augm_flagTrue_train_typeACET_steps40.pth'\n",
    "model = torch.load('Checkpoints/' + file).to(device)\n",
    "\n",
    "train_loader = model_params.train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ACET(model, device, train_loader, noise_loader, optimizer, epoch, lam=1., steps=40, epsilon=0.3, verbose=100):\n",
    "    criterion = nn.NLLLoss()\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    enum = enumerate(train_loader)\n",
    "    for batch_idx, (data, target) in enum:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        noise = torch.rand_like(data)\n",
    "\n",
    "        output = model(data)\n",
    "        \n",
    "        model.eval()\n",
    "        adv_noise = (adv.gen_adv_noise(model, device, noise, epsilon=epsilon, steps=steps) \n",
    "                     + 1e-4*torch.rand_like(noise))\n",
    "        output_adv = model(noise)\n",
    "        model.train()\n",
    "        \n",
    "        loss = criterion(output, target) - output_adv.mean()\n",
    "        #loss = criterion(output, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss)\n",
    "    return train_loss/len(train_loader.dataset), correct/len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay = 5e-4\n",
    "lr = 0.1\n",
    "param_groups = [{'params':model.parameters(),'lr':lr, 'weight_decay':decay}]\n",
    "\n",
    "optimizer = optim.SGD(param_groups, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9302, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.9408, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7733, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8589, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7050, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8155, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8148, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.9159, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8498, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7881, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8100, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7881, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.6120, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7171, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.6264, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7185, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7689, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7672, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7586, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8239, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8208, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7922, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8580, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7057, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7801, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8361, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.9627, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.9557, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7717, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8152, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7735, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7771, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7155, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.6739, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7068, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7953, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7510, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.9388, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8210, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8874, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.9049, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.6635, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8215, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8218, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8311, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.6994, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7297, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7643, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8159, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7464, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7470, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8061, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7957, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7428, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8711, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8083, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8129, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8816, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7984, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7370, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7145, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.6949, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8803, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7688, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.9353, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7838, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8831, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.6249, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7908, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7656, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7888, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.6838, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7715, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7956, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.6682, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8307, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7274, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7579, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7904, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8162, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8121, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7470, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7262, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8150, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8154, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7726, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7571, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8142, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7576, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7349, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7730, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7337, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8452, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7638, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8253, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7728, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7731, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7285, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8427, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8174, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7533, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8063, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7917, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8233, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7177, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8074, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7114, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8319, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8096, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7743, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.6957, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7368, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7430, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8374, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7457, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7817, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7272, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7704, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.6594, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8060, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8075, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8827, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7780, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8904, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.9161, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8407, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7110, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8311, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7194, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7825, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.6988, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.9082, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7469, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.9286, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7494, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7845, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8664, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7849, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8349, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7753, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.9174, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7725, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8514, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8577, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7643, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7736, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8875, device='cuda:5', grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8608, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7080, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8500, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8358, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8323, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8664, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8484, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7908, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8204, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.7914, device='cuda:5', grad_fn=<SubBackward0>)\n",
      "tensor(2.8198, device='cuda:5', grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9b1130b3f352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ACET\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-aaef5d50f449>\u001b[0m in \u001b[0;36mtrain_ACET\u001b[0;34m(model, device, train_loader, noise_loader, optimizer, epoch, lam, steps, epsilon, verbose)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         adv_noise = (adv.gen_adv_noise(model, device, noise, epsilon=epsilon, steps=steps) \n\u001b[0m\u001b[1;32m     17\u001b[0m                      + 1e-4*torch.rand_like(noise))\n\u001b[1;32m     18\u001b[0m         \u001b[0moutput_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/notebooks/gmm-robust/utils/adversarial.py\u001b[0m in \u001b[0;36mgen_adv_noise\u001b[0;34m(model, device, seed, epsilon, steps, step_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_ACET(model, device, train_loader, train_loader, optimizer, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
