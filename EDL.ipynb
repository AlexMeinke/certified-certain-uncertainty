{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import utils.edl as edl\n",
    "import utils.traintest as tt\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import utils.dataloaders as dl\n",
    "\n",
    "import model_params\n",
    "\n",
    "#reload(edl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMNIST\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-4:\n",
      "Process Process-1:\n",
      "Process Process-3:\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/alexm/.local/lib/python3.5/site-packages/torchvision/datasets/mnist.py\", line 95, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/alexm/.local/lib/python3.5/site-packages/torchvision/datasets/mnist.py\", line 95, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/home/alexm/.local/lib/python3.5/site-packages/torchvision/datasets/mnist.py\", line 95, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/home/alexm/.local/lib/python3.5/site-packages/torchvision/transforms/transforms.py\", line 60, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/alexm/.local/lib/python3.5/site-packages/torchvision/datasets/mnist.py\", line 95, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/home/alexm/.local/lib/python3.5/site-packages/torchvision/transforms/transforms.py\", line 60, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/alexm/project/notebooks/gmm-robust/utils/preproc.py\", line 30, in __call__\n",
      "    new_data[i] = (x.view(np.prod(shape[-2:]))[idx]).view(shape[-2:])\n",
      "  File \"/home/alexm/.local/lib/python3.5/site-packages/torchvision/transforms/transforms.py\", line 60, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/alexm/.local/lib/python3.5/site-packages/torchvision/transforms/transforms.py\", line 60, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/alexm/project/notebooks/gmm-robust/utils/preproc.py\", line 39, in __call__\n",
      "    return torch.tensor(filters.gaussian_filter(data, sigma, mode='reflect'))\n",
      "  File \"/home/alexm/.local/lib/python3.5/site-packages/torchvision/transforms/transforms.py\", line 91, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/alexm/project/notebooks/gmm-robust/utils/preproc.py\", line 39, in __call__\n",
      "    return torch.tensor(filters.gaussian_filter(data, sigma, mode='reflect'))\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/scipy/ndimage/filters.py\", line 286, in gaussian_filter\n",
      "    mode, cval, truncate)\n",
      "  File \"/home/alexm/.local/lib/python3.5/site-packages/torchvision/transforms/functional.py\", line 79, in to_tensor\n",
      "    img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/scipy/ndimage/filters.py\", line 287, in gaussian_filter\n",
      "    input = output\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/scipy/ndimage/filters.py\", line 203, in gaussian_filter1d\n",
      "    weights = _gaussian_kernel1d(sigma, order, lw)[::-1]\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/Image.py\", line 727, in tobytes\n",
      "    if len(args) == 1 and isinstance(args[0], tuple):\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/scipy/ndimage/filters.py\", line 140, in _gaussian_kernel1d\n",
      "    x = numpy.arange(-radius, radius + 1)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-f6217e1adf01>\", line 8, in <module>\n",
      "    params = model_params.params_dict[dataset](augm_flag=True)\n",
      "  File \"/home/alexm/project/notebooks/gmm-robust/model_params.py\", line 54, in __init__\n",
      "    ('Noise', dl.Noise(dataset=self.data_name, batch_size=batch_size)),\n",
      "  File \"/home/alexm/project/notebooks/gmm-robust/utils/dataloaders.py\", line 137, in Noise\n",
      "    loader = PrecomputeLoader(loader, batch_size=batch_size, shuffle=True)\n",
      "  File \"/home/alexm/project/notebooks/gmm-robust/utils/dataloaders.py\", line 277, in PrecomputeLoader\n",
      "    for x,l in loader:\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 330, in __next__\n",
      "    idx, batch = self._get_batch()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 309, in _get_batch\n",
      "    return self.data_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1453, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1410, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 672, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 718, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/lib/python3.5/posixpath.py\", line 372, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/usr/lib/python3.5/posixpath.py\", line 406, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/usr/lib/python3.5/posixpath.py\", line 161, in islink\n",
      "    st = os.lstat(path)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 24785) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:6')\n",
    "\n",
    "datasets = ['MNIST', 'FMNIST', 'SVHN', 'CIFAR10', 'CIFAR100']\n",
    "datasets = ['FMNIST', 'SVHN', 'CIFAR10', 'CIFAR100']\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(dataset + '\\n\\n\\n\\n')\n",
    "    params = model_params.params_dict[dataset](augm_flag=True)\n",
    "\n",
    "    if dataset=='MNIST':\n",
    "        base_model = edl.LeNet()\n",
    "    elif dataset=='FMNIST':\n",
    "        base_model = edl.ResNet18(1, 10)\n",
    "    elif dataset in ['SVHN', 'CIFAR10']:\n",
    "        base_model = edl.ResNet18(3, 10)\n",
    "    elif dataset in ['CIFAR100']:\n",
    "        base_model = edl.ResNet18(3, 100)\n",
    "\n",
    "    model = edl.EDL(base_model).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    for epoch in range(100):\n",
    "        if epoch+1 in [50,75,90]:\n",
    "            for group in optimizer.param_groups:\n",
    "                group['lr'] *= .1\n",
    "\n",
    "        trainloss, correct_train = edl.train_edl(model, device, params.train_loader,  \n",
    "                                                 optimizer, epoch, verbose=200)\n",
    "\n",
    "    model.eval()\n",
    "    model = model.to('cpu')\n",
    "\n",
    "    torch.save(model, 'SavedModels/other/edl/' + dataset + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
