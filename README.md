# Towards neural networks that provably know when they don't know

This repository contains the code that was used to obtain the results reported in ARXIV REFERENCE. Using these techniques, one can train models that provably make low-confidence predictions far away from the training data.

## Training the models

Before training a CCU model, one has to first initialize a Gaussian mixture model on the dataset
```
python gen_gmm.py --dataset MNIST --PCA 1 --augm_flag 1
```
All the models in the paper are trained via the script in **run_training.py**. Hyper parameters can be passed as options or edited in **model_params.py**. For example the following lines train a plain model and a CCU on augmented data.

```
python run_training --dataset MNIST --train_type plain --lr 0.001 --augm_flag 1
python run_training --dataset MNIST --train_type CEDA_GMM --lr 0.001 --lr_gmm 1e-5 --augm_flag 1 --use_gmm 1 --PCA 1
```
For all commands (except **gen_gmm.py**) one can specify, which GPU the code should run on via the --gpu option. 

An ODIN model can be generated from any base model by passing the name of the saved model
```
python gen_odin.py --dataset MNIST --SavedModels/base/path base_MNIST_lr0.001_augm_flagTrue_train_typeplain
```

## Testing the models

The out-of-distribution detection statistics are generated by specifying the models one wishes to test in **model_paths.py** and then running
```
python gen_eval.py --dataset MNIST
```

The results of our adversarial noise attack comes from
```
python gen_attack_stats.py --datasets MNIST
```
where mutliple datasets could be specified.
The figures in the paper were generated using the three notebooks.
