{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-87ba536c6e77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mmodel_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugm_flag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m def train(model, device, train_loader, optimizer, epoch, \n",
      "\u001b[0;32m~/project/notebooks/gmm-robust/model_params.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, augm_flag, batch_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m              \u001b[0;34m(\u001b[0m\u001b[0;34m'EMNIST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m              \u001b[0;34m(\u001b[0m\u001b[0;34m'GrayCIFAR10'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGrayCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m              \u001b[0;34m(\u001b[0m\u001b[0;34m'Noise'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m              ('UniformNoise', dl.UniformNoise(self.data_name, batch_size=batch_size))]\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/notebooks/gmm-robust/utils/dataloaders.py\u001b[0m in \u001b[0;36mNoise\u001b[0;34m(dataset, train, batch_size)\u001b[0m\n\u001b[1;32m    134\u001b[0m     loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n\u001b[1;32m    135\u001b[0m                                          shuffle=False, num_workers=4)\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPrecomputeLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/notebooks/gmm-robust/utils/dataloaders.py\u001b[0m in \u001b[0;36mPrecomputeLoader\u001b[0;34m(loader, batch_size, shuffle)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_batch\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import utils.models as models\n",
    "import utils.plotting as plotting\n",
    "import utils.dataloaders as dl\n",
    "import utils.traintest as tt\n",
    "import utils.adversarial as adv\n",
    "import utils.eval as ev\n",
    "import utils.gmm_helpers as gmm_helpers\n",
    "import model_params as params\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import utils.mc_dropout as mc\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "#parser = argparse.ArgumentParser(description='Define hyperparameters.', prefix_chars='-')\n",
    "\n",
    "#parser.add_argument('--gpu', type=int, default=0, help='GPU index.')\n",
    "#parser.add_argument('--dataset', type=str, default='MNIST', help='MNIST, FMNIST, SVHN, CIFAR10, CIFAR100')\n",
    "\n",
    "\n",
    "#hps = parser.parse_args()\n",
    "\n",
    "\n",
    "dataset = 'MNIST'\n",
    "\n",
    "saving_string = dataset + '_base'\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "model_params = params.params_dict[dataset](augm_flag=True)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, \n",
    "          verbose=100, noise_loader=None, epsilon=.3):\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        output = F.log_softmax(model(data), dim=1)\n",
    "        #output = model(data)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        if (batch_idx % verbose == 0) and verbose>0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return train_loss/len(train_loader.dataset), correct/len(train_loader.dataset)\n",
    "\n",
    "\n",
    "if dataset=='MNIST':\n",
    "    model = mc.LeNet()\n",
    "elif dataset=='FMNIST':\n",
    "    model = mc.vgg13(in_channels=1, num_classes=10)\n",
    "elif dataset in ['SVHN', 'CIFAR10']:\n",
    "    model = mc.vgg13(in_channels=3, num_classes=10)\n",
    "elif dataset=='CIFAR100':\n",
    "    model = mc.vgg13(in_channels=3, num_classes=100)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "param_groups = [{'params':model.parameters(),'lr':model_params.lr, 'weight_decay':5e-4}]\n",
    "    \n",
    "if dataset=='MNIST':\n",
    "    optimizer = optim.Adam(param_groups)\n",
    "else: \n",
    "    optimizer = optim.SGD(param_groups, momentum=0.9)\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    if epoch+1 in [50,75,90]:\n",
    "        for group in optimizer.param_groups:\n",
    "            group['lr'] *= .1\n",
    " \n",
    "    trainloss, correct_train = train(model, device, model_params.train_loader,  \n",
    "                                     optimizer, epoch, verbose=-1)\n",
    "    print(str(epoch) + ': \\t' + str(correct_train))\n",
    "\n",
    "model = model.to('cpu')\n",
    "\n",
    "mc_model = mc.MC_Model(model, iterations=10, classes=model_params.classes)\n",
    "\n",
    "torch.save(mc_model, 'SavedModels/other/mcdo/' + dataset + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MC_Model(\n",
       "  (model): LeNet(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (fc1): Linear(in_features=3136, out_features=1024, bias=True)\n",
       "    (fc2): Linear(in_features=1024, out_features=10, bias=True)\n",
       "    (dropout): MC_dropout(p=0.5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(mc)\n",
    "mc_model = mc.MC_Model(model, iterations=7, classes=model_params.classes)\n",
    "mc_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4970)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(mc_model, 'SavedModels/other/mcdo/' + dataset + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(10, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9563,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9895,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5617,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9698,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9924,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7467,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9423,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9794,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9213,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9823,\n",
       "         0.0000]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1000.*mc_model(x)).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(model_params.test_loader))[0]\n",
    "out_data = next(iter(model_params.loaders[0][1]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf, -3.5947e-09,        -inf,        -inf],\n",
       "        [       -inf,        -inf, -3.7253e-05,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf, -4.7048e-06,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [-2.2564e-04,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf, -4.4497e-07,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf, -5.5477e-07,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf, -2.7228e-05,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf, -2.6306e-03],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "         -1.7423e+00,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf, -3.6500e-04],\n",
       "        [-8.0702e-05,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf, -4.2973e-03,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf, -3.1490e-04],\n",
       "        [-1.2764e-04,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf, -1.9987e-06,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "         -1.1256e-02,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf, -9.4132e-04],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf, -1.6824e-06,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf, -1.0003e-02,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf, -2.0664e-06,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf, -7.0897e-03],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf, -2.5729e-07,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf, -1.1516e-03,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "         -2.7359e-05,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf, -1.0354e-04,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [-2.7719e-04,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf, -4.8088e-06,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf, -2.9016e-10,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [-2.1362e-03,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf, -3.3105e-05,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf, -2.1756e-08,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf, -2.8105e-04,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf, -3.0867e-07,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf, -5.8636e-05,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf, -1.5017e-09,        -inf,        -inf],\n",
       "        [       -inf,        -inf, -1.7992e-03,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf, -2.1974e-04,        -inf,        -inf],\n",
       "        [       -inf, -2.9074e-05,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf, -6.0161e-03,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf, -4.1546e-08,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf, -1.8858e-02,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf, -4.7156e-03,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf, -1.9037e-06,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf, -2.4634e-02,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf, -3.8322e-04,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "         -5.3197e-05,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf, -1.6494e-03,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf, -1.7596e-07,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf, -1.2204e-04,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf, -3.2625e-06,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf, -3.1763e-06,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf, -1.7847e-05,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "         -6.9020e-07,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "         -9.8747e-04,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf, -3.1524e-05,        -inf,        -inf,        -inf],\n",
       "        [-4.6519e-05,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf, -1.2041e-11,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf, -7.8917e-06,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf, -6.6992e-05],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "         -1.2598e-03,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf, -1.4974e-08,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf, -9.3089e-07,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf, -7.7195e-01],\n",
       "        [       -inf,        -inf,        -inf, -2.8492e-01,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf, -5.7191e-07,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf, -3.2195e-04,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf, -3.8414e-02,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf, -7.1425e-02,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf, -2.6825e-08,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [-2.3470e-05,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf, -1.3455e-03,        -inf,        -inf],\n",
       "        [-1.3668e-07,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf, -9.4215e-07,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf, -2.5586e-03],\n",
       "        [       -inf, -2.6580e-05,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf, -5.1339e-05,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf, -6.8476e-08,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf, -4.2176e-05,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf, -2.2622e+00],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf, -2.8994e-07,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf, -1.1792e-02,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf, -1.7778e-07,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf, -9.7399e-07,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf, -6.9830e-06,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf, -6.8501e-03,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf, -7.3471e-06,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf, -1.0973e-06,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf, -3.2688e-01,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf, -2.6395e-06,        -inf,        -inf,        -inf],\n",
       "        [       -inf, -3.3024e-05,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf, -1.0351e-07,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf, -3.5380e-06,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf, -4.6889e-02],\n",
       "        [       -inf,        -inf,        -inf, -3.3169e-04,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf, -7.3986e-06,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf, -1.1770e-02,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf, -5.5380e-02,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf, -3.8815e-09,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf, -5.8911e-05,        -inf,        -inf,        -inf],\n",
       "        [       -inf,        -inf,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf, -1.1600e-05]],\n",
       "       grad_fn=<IndexPutBackward>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.4988, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_model(x).max(1)[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-6.4220, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_model(out_data).max(1)[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -5.3726,  -7.9268,  -4.8141,  -7.6775,  -6.1880,  -7.2239,  -4.5580,\n",
       "          -9.6222,  -0.0298,  -6.0110],\n",
       "        [ -4.0340,  -6.3577,  -2.2455,  -5.0191,  -4.6770,  -6.3202,  -2.3114,\n",
       "          -6.5509,  -0.2848,  -5.4524],\n",
       "        [ -5.2128,  -6.9789,  -3.8566,  -5.2700,  -6.3310,  -7.1379,  -3.7537,\n",
       "          -8.5233,  -0.0652,  -5.4509],\n",
       "        [ -6.4082,  -8.8897,  -3.7003,  -6.9389,  -7.3992,  -8.7028,  -4.6785,\n",
       "         -10.2759,  -0.0408,  -6.0173],\n",
       "        [ -3.8536,  -6.0161,  -2.8637,  -3.7964,  -5.4262,  -5.1880,  -2.9985,\n",
       "          -7.1351,  -0.1996,  -4.0651],\n",
       "        [ -5.1228,  -6.6619,  -4.1636,  -5.2017,  -5.4402,  -6.0420,  -4.3007,\n",
       "          -7.0575,  -0.0623,  -4.5131],\n",
       "        [ -4.2476,  -5.9894,  -3.0897,  -5.1981,  -4.7528,  -5.6720,  -3.7552,\n",
       "          -6.8968,  -0.1245,  -4.3644],\n",
       "        [ -3.8885,  -5.1258,  -2.5800,  -4.3765,  -4.1712,  -4.9954,  -2.9889,\n",
       "          -5.7543,  -0.2370,  -3.8859],\n",
       "        [ -4.5326,  -6.3336,  -3.2776,  -5.1054,  -4.6792,  -5.4274,  -3.1316,\n",
       "          -6.6649,  -0.1270,  -5.4284],\n",
       "        [ -4.2652,  -5.9849,  -1.9099,  -5.0449,  -6.6527,  -7.0331,  -4.4589,\n",
       "          -6.7538,  -0.2148,  -4.9265]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_model.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9408, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = []\n",
    "for _ in range(7):\n",
    "    out.append(mc_model.model(data).exp())\n",
    "out = torch.stack(out)\n",
    "y = out.mean(0)\n",
    "\n",
    "1000*((out - y[None,:,:])**2).mean(0).sum(1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import utils.models as models\n",
    "import utils.plotting as plotting\n",
    "import utils.dataloaders as dl\n",
    "import utils.traintest as tt\n",
    "import utils.adversarial as adv\n",
    "import utils.eval as ev\n",
    "import utils.gmm_helpers as gmm_helpers\n",
    "import model_params as params\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import utils.mc_dropout as mc\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multindex = pd.DataFrame(stats_dict,\n",
    "                             index=[len(metrics)*['MNIST'],\n",
    "                                    metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrr}\n",
      "\\toprule\n",
      "      &     &       CCU &      base \\\\\n",
      "\\midrule\n",
      "MNIST & TE &  0.999426 &  0.428961 \\\\\n",
      "      & MMC &  0.789796 &  0.878765 \\\\\n",
      "      & SR &  0.310605 &  0.268120 \\\\\n",
      "      & AUC &  0.114866 &  0.535991 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_multindex.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = torch.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['base', 'CCU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base', 'CCU']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_stats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = torch.rand(2, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('base',\n",
       "              [0.4289613962173462,\n",
       "               0.8787645101547241,\n",
       "               0.26811957359313965,\n",
       "               0.535990834236145]),\n",
       "             ('CCU',\n",
       "              [0.9994264245033264,\n",
       "               0.7897958755493164,\n",
       "               0.31060510873794556,\n",
       "               0.11486554145812988])])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_dict = collections.OrderedDict(zip(models, stats[:,:,0].tolist()))\n",
    "stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\n",
      "CCU\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(models, stats[:,:,0].tolist()):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4290],\n",
       "         [0.8788],\n",
       "         [0.2681],\n",
       "         [0.5360]],\n",
       "\n",
       "        [[0.9994],\n",
       "         [0.7898],\n",
       "         [0.3106],\n",
       "         [0.1149]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'SavedModels/other/mcdo/FMNIST.pth'\n",
    "model = torch.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'results/backup/samples_steps500_alpha3.0_restarts50_batches2_batch_size50_FMNIST_2019-09-09 14:19:01.427238_FMNIST.pth'\n",
    "stats = torch.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 1, 28, 28])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.seeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-5.8868e-05, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(stats.seeds[0]).max(1)[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(stats.samples[0,0]).max(1)[0].exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'SavedModels/other/mcdo/MNIST.pth'\n",
    "model = torch.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.mc_dropout' from '/home/alexm/project/notebooks/gmm-robust/utils/mc_dropout.py'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.8848e+00, -1.1595e+01, -6.0736e+00, -9.5124e+00, -9.5232e+00,\n",
       "         -9.6294e+00, -7.7080e+00, -1.2109e+01, -3.5944e-03, -8.3685e+00],\n",
       "        [-8.5293e+00, -1.0281e+01, -6.0877e+00, -7.1184e+00, -8.3114e+00,\n",
       "         -7.4821e+00, -7.1142e+00, -1.1321e+01, -5.1584e-03, -8.5258e+00],\n",
       "        [-8.0917e+00, -1.1280e+01, -7.2941e+00, -8.9765e+00, -7.9143e+00,\n",
       "         -9.8949e+00, -9.6573e+00, -1.1163e+01, -2.4381e-03, -7.1104e+00],\n",
       "        [-7.1958e+00, -9.4005e+00, -4.7116e+00, -7.9325e+00, -5.7638e+00,\n",
       "         -8.7296e+00, -7.2347e+00, -9.8245e+00, -1.4677e-02, -8.0719e+00],\n",
       "        [-1.1978e+01, -1.3703e+01, -9.5455e+00, -1.0807e+01, -1.1731e+01,\n",
       "         -1.0837e+01, -9.4580e+00, -1.4891e+01, -2.1172e-04, -1.1890e+01],\n",
       "        [-1.0263e+01, -1.3445e+01, -8.7900e+00, -9.9919e+00, -1.1504e+01,\n",
       "         -1.0646e+01, -9.1092e+00, -1.5402e+01, -3.9101e-04, -1.1337e+01],\n",
       "        [-9.1353e+00, -1.1820e+01, -8.2553e+00, -9.3483e+00, -9.7158e+00,\n",
       "         -9.2500e+00, -7.9322e+00, -1.3153e+01, -1.0996e-03, -9.0312e+00],\n",
       "        [-8.6961e+00, -1.1773e+01, -6.9860e+00, -9.2141e+00, -1.1183e+01,\n",
       "         -1.1864e+01, -8.4501e+00, -1.2520e+01, -1.4629e-03, -1.0631e+01],\n",
       "        [-9.2485e+00, -1.0725e+01, -7.4647e+00, -7.8567e+00, -9.2922e+00,\n",
       "         -9.7957e+00, -7.9230e+00, -1.2086e+01, -1.8244e-03, -8.3855e+00],\n",
       "        [-8.9166e+00, -1.1581e+01, -8.0000e+00, -8.9992e+00, -9.0028e+00,\n",
       "         -9.8535e+00, -8.9183e+00, -1.2814e+01, -1.1940e-03, -8.1874e+00]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \t0.5813666666666667\n",
      "1: \t0.7779833333333334\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-301a2ee7ac0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     trainloss, correct_train = train(model, device, model_params.train_loader,  \n\u001b[0;32m---> 97\u001b[0;31m                                      optimizer, epoch, verbose=-1)\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': \\t'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-301a2ee7ac0c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, verbose, noise_loader, epsilon)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import utils.models as models\n",
    "import utils.plotting as plotting\n",
    "import utils.dataloaders as dl\n",
    "import utils.traintest as tt\n",
    "import utils.adversarial as adv\n",
    "import utils.eval as ev\n",
    "import utils.gmm_helpers as gmm_helpers\n",
    "import model_params as params\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import utils.mc_dropout as mc\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "dataset = 'FMNIST'\n",
    "\n",
    "saving_string = dataset + '_base'\n",
    "device = torch.device('cuda:4')\n",
    "\n",
    "model_params = params.params_dict[dataset](augm_flag=True)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, \n",
    "          verbose=100, noise_loader=None, epsilon=.3):\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        #output = F.log_softmax(model(data), dim=1)\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        if (batch_idx % verbose == 0) and verbose>0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return train_loss/len(train_loader.dataset), correct/len(train_loader.dataset)\n",
    "\n",
    "\n",
    "if dataset=='MNIST':\n",
    "    model = mc.LeNet()\n",
    "elif dataset=='FMNIST':\n",
    "    model = mc.vgg13(in_channels=1, num_classes=10)\n",
    "elif dataset in ['SVHN', 'CIFAR10']:\n",
    "    model = mc.vgg13(in_channels=3, num_classes=10)\n",
    "elif dataset=='CIFAR100':\n",
    "    model = mc.vgg13(in_channels=3, num_classes=100)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "lr = model_params.lr\n",
    "lr = 2e-4\n",
    "param_groups = [{'params':model.parameters(),'lr':lr, 'weight_decay':0.}]\n",
    "    \n",
    "if dataset=='MNIST':\n",
    "    optimizer = optim.Adam(param_groups)\n",
    "else: \n",
    "    optimizer = optim.SGD(param_groups, momentum=0.9)\n",
    "\n",
    "    \n",
    "optimizer = optim.Adam(param_groups)\n",
    "\n",
    "for epoch in range(100):\n",
    "    if epoch+1 in [50,75,90]:\n",
    "        for group in optimizer.param_groups:\n",
    "            group['lr'] *= .1\n",
    " \n",
    "    trainloss, correct_train = train(model, device, model_params.train_loader,  \n",
    "                                     optimizer, epoch, verbose=-1)\n",
    "    print(str(epoch) + ': \\t' + str(correct_train))\n",
    "\n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = mc.MC_Model(model, iterations=10, classes=model_params.classes)\n",
    "\n",
    "torch.save(mc_model, 'SavedModels/other/mcdo/' + dataset + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model_params' from '/home/alexm/project/notebooks/gmm-robust/model_params.py'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1001, 0.1009, 0.1009, 0.1015, 0.0988, 0.1009, 0.1009, 0.0957, 0.0985,\n",
       "         0.1019],\n",
       "        [0.1001, 0.1009, 0.1009, 0.1014, 0.0987, 0.1008, 0.1009, 0.0957, 0.0986,\n",
       "         0.1019],\n",
       "        [0.1001, 0.1008, 0.1011, 0.1015, 0.0988, 0.1008, 0.1008, 0.0957, 0.0985,\n",
       "         0.1020],\n",
       "        [0.1001, 0.1008, 0.1009, 0.1014, 0.0988, 0.1008, 0.1009, 0.0957, 0.0986,\n",
       "         0.1020],\n",
       "        [0.1001, 0.1009, 0.1010, 0.1014, 0.0988, 0.1008, 0.1009, 0.0957, 0.0985,\n",
       "         0.1020],\n",
       "        [0.1000, 0.1009, 0.1009, 0.1014, 0.0988, 0.1009, 0.1009, 0.0957, 0.0986,\n",
       "         0.1019],\n",
       "        [0.1001, 0.1009, 0.1010, 0.1014, 0.0988, 0.1008, 0.1008, 0.0957, 0.0985,\n",
       "         0.1019],\n",
       "        [0.1001, 0.1009, 0.1010, 0.1015, 0.0988, 0.1008, 0.1008, 0.0958, 0.0985,\n",
       "         0.1019],\n",
       "        [0.1001, 0.1009, 0.1009, 0.1014, 0.0988, 0.1008, 0.1008, 0.0958, 0.0985,\n",
       "         0.1019],\n",
       "        [0.1001, 0.1009, 0.1010, 0.1014, 0.0988, 0.1008, 0.1008, 0.0957, 0.0985,\n",
       "         0.1019]], device='cuda:4', grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.rand(10,1,28,28,device=device)).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.dataloaders' from '/home/alexm/project/notebooks/gmm-robust/utils/dataloaders.py'>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = dl.TinyImages('MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6662, 0.2200, 0.8140, 0.3196, 0.8920, 0.0562, 0.4096, 0.1929, 0.5605,\n",
       "        0.1893])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3196)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('SavedModels/other/mcdo/SVHN.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU(inplace)\n",
       "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): ReLU(inplace)\n",
       "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): ReLU(inplace)\n",
       "    (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MC_dropout(p=0.5)\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MC_dropout(p=0.5)\n",
       "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.maha_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import utils.models as models\n",
    "import utils.plotting as plotting\n",
    "import utils.dataloaders as dl\n",
    "import utils.traintest as tt\n",
    "import utils.adversarial as adv\n",
    "import utils.eval as ev\n",
    "import utils.gmm_helpers as gmm_helpers\n",
    "import model_params as params\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import utils.mc_dropout as mc\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:7')\n",
    "model = torch.load('SavedModels/other/mcdo/MNIST.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8804e-03, 1.7448e-04, 7.3236e-01, 9.2889e-03, 1.6873e-02, 1.6071e-03,\n",
       "         2.3579e-01, 6.3311e-05, 9.3219e-04, 3.3302e-05],\n",
       "        [4.1709e-05, 3.6959e-06, 1.4710e-01, 3.0959e-05, 4.8656e-03, 9.5192e-07,\n",
       "         8.4768e-01, 2.1659e-07, 2.7373e-04, 2.0173e-07],\n",
       "        [7.7012e-07, 1.3498e-08, 3.6277e-02, 2.6285e-03, 1.5259e-03, 4.0689e-05,\n",
       "         9.5952e-01, 3.0530e-07, 2.0190e-06, 3.8262e-08],\n",
       "        [6.5478e-03, 2.8062e-03, 8.3580e-02, 2.8812e-01, 8.9296e-03, 7.8002e-03,\n",
       "         5.8434e-01, 8.3119e-04, 3.4306e-03, 1.3612e-02],\n",
       "        [1.1907e-03, 1.8654e-04, 1.1173e-01, 3.3944e-02, 1.1603e-02, 1.1087e-03,\n",
       "         8.3966e-01, 8.8793e-05, 1.7819e-04, 3.0764e-04],\n",
       "        [1.2291e-04, 9.0490e-05, 3.6043e-02, 5.3851e-03, 1.9022e-03, 1.0737e-04,\n",
       "         9.5611e-01, 2.0288e-06, 1.6293e-04, 7.5208e-05],\n",
       "        [1.8380e-06, 3.3532e-06, 1.4044e-02, 5.0767e-03, 1.5441e-04, 8.0171e-06,\n",
       "         9.8069e-01, 1.8511e-07, 1.9558e-05, 5.6284e-06],\n",
       "        [2.0336e-04, 5.3181e-05, 6.3004e-03, 1.0613e-03, 2.1259e-02, 1.7099e-04,\n",
       "         9.7082e-01, 1.6872e-05, 7.8380e-06, 1.0327e-04],\n",
       "        [2.3524e-06, 5.7690e-07, 9.1188e-02, 2.0839e-03, 1.3008e-04, 3.5841e-05,\n",
       "         9.0654e-01, 2.3144e-07, 1.9894e-05, 1.1314e-07],\n",
       "        [8.0347e-06, 1.7248e-05, 2.3036e-03, 4.4428e-03, 1.9515e-03, 2.4611e-05,\n",
       "         9.9122e-01, 1.2041e-06, 1.4560e-06, 2.8027e-05]],\n",
       "       grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model(torch.rand(10,3,32,32)).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = next(iter(dl.MNIST(train=False)))\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = []\n",
    "for i in range(500):\n",
    "    vals.append(model.model(data).exp()[1,[8, 2, 6]].detach().cpu())\n",
    "vals = torch.stack(vals, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    plt.hist(vals[:,i], density=True, alpha=.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(20, 3, 32, 32, device=device)\n",
    "\n",
    "vals_x = []\n",
    "for i in range(500):\n",
    "    vals_x.append(model.model(x).exp()[0,[8, 2, 6]].detach().cpu())\n",
    "vals_x = torch.stack(vals_x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAEkFJREFUeJzt3X+wXGd93/H3J5YxaaHYoBuPK4nITURSh06E59aYodOC3STG7SBnSjz2NKAwapUG0yGFpjHpH5C2HsK04JaZ2KmoXEQmwXZJUmuI88O1xXjoYJNrEIp/hOTGGCxVWDfGdmA8uLX59o99VC6qpN17d/eu7+P3a2Znz3nOc/Z8H/343HOfPbsnVYUkqV/fM+sCJEnTZdBLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOrdh1gUAbNy4sbZu3TrrMiRpXbnvvvv+oqrmhvV7XgT91q1bWVhYmHUZkrSuJPnKKP2cupGkzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM6t+6C//o4/nXUJkvS8tu6DXpJ0ega9JHXOoJekzo0c9EnOSPKFJJ9q6+cnuTfJYpJbkryotZ/V1hfb9q3TKV2SNIqVnNG/C3ho2foHgeur6geBJ4BdrX0X8ERrv771kyTNyEhBn2Qz8A+A/9LWA1wCfLJ12Qdc0ZZ3tHXa9ktbf0nSDIx6Rv8fgX8FfLutvwJ4sqqebeuHgU1teRPwKEDb/lTrL0magaFBn+QfAseq6r5JHjjJ7iQLSRaWlpYm+dKSpGVGOaN/PfDmJI8ANzOYsvlPwNlJjt+KcDNwpC0fAbYAtO0vAx4/8UWrak9VzVfV/Nzc0FseSpJWaWjQV9V7q2pzVW0FrgLuqqp/DBwA3tK67QRua8v72zpt+11VVROtWpI0snGuo/9F4N1JFhnMwe9t7XuBV7T2dwPXjleiJGkcG4Z3+Y6q+jTw6bb8MHDRSfp8C/ipCdQmSZoAPxkrSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SercKDcHf3GSzyX5YpIHkvxya/9Yki8nOdge21t7knwkyWKSQ0kunOYALv7qnmm+vCSte6PcYeoZ4JKq+maSM4HPJPm9tu0XquqTJ/R/E7CtPV4L3NieJUkzMMrNwauqvtlWz2yP093sewfw8bbfPcDZSc4bv1RJ0mqMNEef5IwkB4FjwB1VdW/bdF2bnrk+yVmtbRPw6LLdD7c2SdIMjBT0VfVcVW0HNgMXJXk18F7gh4G/Dbwc+MWVHDjJ7iQLSRaWlpZWWLYkaVQruuqmqp4EDgCXVdXRNj3zDPBfgYtatyPAlmW7bW5tJ77Wnqqar6r5ubm51VUvSRpqlKtu5pKc3Za/F/gx4E+Oz7snCXAFcH/bZT/wtnb1zcXAU1V1dCrVS5KGGuWqm/OAfUnOYPCD4daq+lSSu5LMAQEOAv+s9b8duBxYBJ4G3j75siVJoxoa9FV1CHjNSdovOUX/Aq4ZvzRJ0iT4yVhJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUudGuZXgi5N8LskXkzyQ5Jdb+/lJ7k2ymOSWJC9q7We19cW2fet0hyBJOp1RzuifAS6pqh8FtgOXtXvBfhC4vqp+EHgC2NX67wKeaO3Xt36SpBkZGvQ18M22emZ7FHAJ8MnWvo/BDcIBdrR12vZL2w3EJUkzMNIcfZIzkhwEjgF3AH8OPFlVz7Yuh4FNbXkT8ChA2/4U8IqTvObuJAtJFpaWlsYbhSTplEYK+qp6rqq2A5uBi4AfHvfAVbWnquaran5ubm7cl5MkncKKrrqpqieBA8DrgLOTbGibNgNH2vIRYAtA2/4y4PGJVCtJWrFRrrqZS3J2W/5e4MeAhxgE/ltat53AbW15f1unbb+rqmqSRUuSRrdheBfOA/YlOYPBD4Zbq+pTSR4Ebk7y74AvAHtb/73ArydZBL4OXDWFuiVJIxoa9FV1CHjNSdofZjBff2L7t4Cfmkh1kqSx+clYSeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnRrmV4JYkB5I8mOSBJO9q7e9PciTJwfa4fNk+702ymORLSX5imgOQJJ3eKLcSfBZ4T1V9PslLgfuS3NG2XV9V/2F55yQXMLh94I8Afx34H0leVVXPTbJwSdJohp7RV9XRqvp8W/4GgxuDbzrNLjuAm6vqmar6MrDISW45KElaGyuao0+ylcH9Y+9tTe9McijJTUnOaW2bgEeX7XaYk/xgSLI7yUKShaWlpRUXLkkazchBn+QlwG8BP19VfwncCPwAsB04CnxoJQeuqj1VNV9V83NzcyvZVZK0AiMFfZIzGYT8b1TVbwNU1WNV9VxVfRv4KN+ZnjkCbFm2++bWJkmagVGuugmwF3ioqj68rP28Zd1+Eri/Le8HrkpyVpLzgW3A5yZXsiRpJUa56ub1wFuBP05ysLX9EnB1ku1AAY8APwtQVQ8kuRV4kMEVO9d4xY0kzc7QoK+qzwA5yabbT7PPdcB1Y9QlSZoQPxkrSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SercKLcS3JLkQJIHkzyQ5F2t/eVJ7kjyZ+35nNaeJB9JspjkUJILpz0ISdKpjXJG/yzwnqq6ALgYuCbJBcC1wJ1VtQ24s60DvInBfWK3AbuBGydetSRpZEODvqqOVtXn2/I3gIeATcAOYF/rtg+4oi3vAD5eA/cAZ59wI3FJ0hpa0Rx9kq3Aa4B7gXOr6mjb9DXg3La8CXh02W6HW9uJr7U7yUKShaWlpRWWLUka1chBn+QlwG8BP19Vf7l8W1UVUCs5cFXtqar5qpqfm5tbya6SpBUYKeiTnMkg5H+jqn67NT92fEqmPR9r7UeALct239zaJEkzMMpVNwH2Ag9V1YeXbdoP7GzLO4HblrW/rV19czHw1LIpHknSGtswQp/XA28F/jjJwdb2S8CvALcm2QV8BbiybbsduBxYBJ4G3j7RiiVJKzI06KvqM0BOsfnSk/Qv4Jox65IkTYifjJWkzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glaUZuOHjDmhzHoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1bpQ7TN2U5FiS+5e1vT/JkSQH2+PyZdvem2QxyZeS/MS0CpckjWaUM/qPAZedpP36qtreHrcDJLkAuAr4kbbPDUnOmFSxkqSVGxr0VXU38PURX28HcHNVPVNVX2ZwO8GLxqhPkjSmcebo35nkUJvaOae1bQIeXdbncGuTJM3IaoP+RuAHgO3AUeBDK32BJLuTLCRZWFpaWmUZkqRhVhX0VfVYVT1XVd8GPsp3pmeOAFuWdd3c2k72Gnuqar6q5ufm5lZThiRpBKsK+iTnLVv9SeD4FTn7gauSnJXkfGAb8LnxSpQkjWPDsA5JPgG8AdiY5DDwPuANSbYDBTwC/CxAVT2Q5FbgQeBZ4Jqqem46pUuSRjE06Kvq6pM07z1N/+uA68YpSpI0OX4yVpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUuaFBn+SmJMeS3L+s7eVJ7kjyZ+35nNaeJB9JspjkUJILp1m8JGm4Uc7oPwZcdkLbtcCdVbUNuLOtA7yJwX1itwG7gRsnU6YkabWGBn1V3Q18/YTmHcC+trwPuGJZ+8dr4B7g7BNuJC5JWmOrnaM/t6qOtuWvAee25U3Ao8v6HW5tkqQZGfvN2KoqoFa6X5LdSRaSLCwtLY1bhiTpFFYb9I8dn5Jpz8da+xFgy7J+m1vb/6eq9lTVfFXNz83NrbIMSdIwqw36/cDOtrwTuG1Z+9va1TcXA08tm+KRJM3AhmEdknwCeAOwMclh4H3ArwC3JtkFfAW4snW/HbgcWASeBt4+hZolSSswNOir6upTbLr0JH0LuGbcoiRJk+MnYyWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjrXR9Af+MCsK5Ck560+gl6SdEoGvSR1buiNR04nySPAN4DngGeraj7Jy4FbgK3AI8CVVfXEeGVKklZrEmf0b6yq7VU139avBe6sqm3AnW1dkjQj05i62QHsa8v7gCumcAxJ0ojGDfoC/jDJfUl2t7Zzq+poW/4acO6Yx5AkjWGsOXrg71TVkSTfB9yR5E+Wb6yqSlIn27H9YNgN8MpXvnLMMiRJpzLWGX1VHWnPx4DfAS4CHktyHkB7PnaKffdU1XxVzc/NzY1ThiTpNFYd9En+apKXHl8Gfhy4H9gP7GzddgK3jVvkMJ99+PFpH0KS1q1xpm7OBX4nyfHX+c2q+v0kfwTcmmQX8BXgyvHLlCSt1qqDvqoeBn70JO2PA5eOU5QkaXLGfTNWkrRCNxy8YU2P51cgSFLnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalz/QT9gQ/MugJJGmqtv9AMegp6SdJJGfSS1LmpBX2Sy5J8KclikmundZzjvJ2gpOe7WUzbwJSCPskZwK8CbwIuAK5OcsE0jrXcZ/f+y2kfQpLWnWmd0V8ELFbVw1X1v4GbgR1TOtZ3O/6mrG/OSnoemdXZPEzvVoKbgEeXrR8GXjulY32Xzz78OK/jA4PnN67FESVp4HiYv2P7O75rfdZSVZN/0eQtwGVV9U/a+luB11bVO5f12Q3sbqs/BHxplYfbCPzFGOWuR475hcExvzCMM+bvr6q5YZ2mdUZ/BNiybH1za/t/qmoPsGfcAyVZqKr5cV9nPXHMLwyO+YVhLcY8rTn6PwK2JTk/yYuAq4D9UzqWJOk0pnJGX1XPJnkn8AfAGcBNVfXANI4lSTq9aU3dUFW3A7dP6/WXGXv6Zx1yzC8MjvmFYepjnsqbsZKk5w+/AkGSOrdugn7YVyokOSvJLW37vUm2rn2VkzXCmN+d5MEkh5LcmeT7Z1HnJI361RlJ/lGSSrLur9AYZcxJrmx/1w8k+c21rnHSRvi3/cokB5J8of37vnwWdU5KkpuSHEty/ym2J8lH2p/HoSQXTrSAqnrePxi8ofvnwN8AXgR8EbjghD7vAH6tLV8F3DLrutdgzG8E/kpb/rkXwphbv5cCdwP3APOzrnsN/p63AV8Azmnr3zfrutdgzHuAn2vLFwCPzLruMcf8d4ELgftPsf1y4PeAABcD907y+OvljH6Ur1TYAexry58ELk2SNaxx0oaOuaoOVNXTbfUeBp9XWM9G/eqMfwt8EPjWWhY3JaOM+Z8Cv1pVTwBU1bE1rnHSRhlzAX+tLb8M+F9rWN/EVdXdwNdP02UH8PEauAc4O8l5kzr+egn6k32lwqZT9amqZ4GngFesSXXTMcqYl9vF4IxgPRs65vYr7Zaq+t21LGyKRvl7fhXwqiT/M8k9SS5bs+qmY5Qxvx/46SSHGVy998/XprSZWen/9xWZ2uWVWjtJfhqYB/7erGuZpiTfA3wY+JkZl7LWNjCYvnkDg9/a7k7yt6rqyZlWNV1XAx+rqg8leR3w60leXVXfnnVh69F6OaMf+pUKy/sk2cDg1731/CX1o4yZJH8f+NfAm6vqmTWqbVqGjfmlwKuBTyd5hMFc5v51/obsKH/Ph4H9VfV/qurLwJ8yCP71apQx7wJuBaiqzwIvZvCdML0a6f/7aq2XoB/lKxX2Azvb8luAu6q9y7FODR1zktcA/5lByK/3eVsYMuaqeqqqNlbV1qrayuB9iTdX1cJsyp2IUf5t/3cGZ/Mk2chgKufhtSxywkYZ81eBSwGS/E0GQb+0plWurf3A29rVNxcDT1XV0Um9+LqYuqlTfKVCkn8DLFTVfmAvg1/vFhm86XHV7Coe34hj/vfAS4D/1t53/mpVvXlmRY9pxDF3ZcQx/wHw40keBJ4DfqGq1u1vqyOO+T3AR5P8CwZvzP7Mej5xS/IJBj+sN7b3Hd4HnAlQVb/G4H2Iy4FF4Gng7RM9/jr+s5MkjWC9TN1IklbJoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXP/Fx8tJs0afqcZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    plt.hist(vals_x[:,i], density=True, alpha=.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 499.]),\n",
       " array([0.99999422, 0.9999948 , 0.99999537, 0.99999595, 0.99999653,\n",
       "        0.99999711, 0.99999769, 0.99999827, 0.99999884, 0.99999942,\n",
       "        1.        ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEJCAYAAAB8Pye7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAEiRJREFUeJzt3X+UZ3Vdx/HnC1YwtOTXSraLDiaaqIG2Ih6rQ5IlaC6n/AGVbkbuqai0X4r9OFmnOtgv0nNM5YiyVv5AzUDjpLhaliW6KCA/IgaC2A3dBYEysiLf/fH9bH532tnvd2a+48x3Ps/HOXPmcz/3c+/9vGd2XnP3fr/3TqoKSVJfDlrpCUiSvvYMf0nqkOEvSR0y/CWpQ4a/JHXI8JekDhn+ktQhw1+SOmT4S1KH1q30BACOPvrompmZWelpSNJUueqqq+6qqvWL2XZVhP/MzAw7duxY6WlI0lRJcvtit/WyjyR1yPCXpA4Z/pLUIcNfkjpk+EtSh8YK/yS3JflckquT7Gh9Rya5IsnN7fMRrT9JXp9kNsm1SZ6ynAVIkhZuIWf+31VVJ1XVprZ8HrC9qo4HtrdlgNOB49vHVuCNk5qsJGkylnLZZzOwrbW3AWcO9b+9Bj4JHJ7kEUs4jiRpwsYN/wI+nOSqJFtb3zFVdWdrfx44prU3AHcMbbuz9e0jydYkO5Ls2LNnzyKmLklarHHv8P32qtqV5OHAFUn+YXhlVVWSBf0l+Kq6ELgQYNOmTf4VeUkrZua8v1ixY992/nNW5LhjnflX1a72eTfwfuBk4At7L+e0z7vb8F3AsUObb2x9kqRVYmT4J3lIkq/f2wa+B7gOuAzY0oZtAS5t7cuAl7R3/ZwC3Dd0eUiStAqMc9nnGOD9SfaOf0dV/WWSTwOXJDkHuB14YRt/OXAGMAvcD7x04rOWJC3JyPCvqluBE/fTfzdw2n76Czh3IrOTJC0L7/CVpA4Z/pLUIcNfkjpk+EtShwx/SeqQ4S9JHTL8JalDhr8kdcjwl6QOGf6S1CHDX5I6ZPhLUocMf0nqkOEvSR0y/CWpQ4a/JHXI8JekDhn+ktQhw1+SOmT4S1KHDH9J6pDhL0kdMvwlqUOGvyR1yPCXpA4Z/pLUIcNfkjpk+EtShwx/SeqQ4S9JHTL8JalDhr8kdWjs8E9ycJLPJvlgWz4uyZVJZpO8O8khrf/Qtjzb1s8sz9QlSYu1kDP/lwM3Di2/Frigqh4D3AOc0/rPAe5p/Re0cZKkVWSs8E+yEXgO8Ja2HOCZwHvbkG3Ama29uS3T1p/WxkuSVolxz/z/EHgl8JW2fBRwb1U90JZ3AhtaewNwB0Bbf18bL0laJUaGf5LnArur6qpJHjjJ1iQ7kuzYs2fPJHctSRphnDP/ZwDPS3Ib8C4Gl3teBxyeZF0bsxHY1dq7gGMB2vqHAXfP3WlVXVhVm6pq0/r165dUhCRpYUaGf1W9uqo2VtUMcBbw0ar6IeBjwPPbsC3Apa19WVumrf9oVdVEZy1JWpKlvM//VcDPJZllcE3/otZ/EXBU6/854LylTVGSNGnrRg/5qqr6K+CvWvtW4OT9jPky8IIJzE2StEy8w1eSOmT4S1KHDH9J6pDhL0kdMvwlqUOGvyR1yPCXpA4Z/pLUIcNfkjpk+EtShwx/SeqQ4S9JHTL8JalDhr8kdcjwl6QOGf6S1CHDX5I6ZPhLUocMf0nqkOEvSR0y/CWpQ4a/JHXI8JekDhn+ktQhw1+SOmT4S1KHDH9J6pDhL0kdMvwlqUOGvyR1yPCXpA4Z/pLUIcNfkjo0MvyTPDjJp5Jck+T6JL/e+o9LcmWS2STvTnJI6z+0Lc+29TPLW4IkaaHGOfP/T+CZVXUicBLw7CSnAK8FLqiqxwD3AOe08ecA97T+C9o4SdIqMjL8a+BLbfFB7aOAZwLvbf3bgDNbe3Nbpq0/LUkmNmNJ0pKNdc0/ycFJrgZ2A1cAtwD3VtUDbchOYENrbwDuAGjr7wOO2s8+tybZkWTHnj17llaFJGlBxgr/qvqfqjoJ2AicDHzLUg9cVRdW1aaq2rR+/fql7k6StAALerdPVd0LfAx4OnB4knVt1UZgV2vvAo4FaOsfBtw9kdlKkiZinHf7rE9yeGt/HfAs4EYGvwSe34ZtAS5t7cvaMm39R6uqJjlpSdLSrBs9hEcA25IczOCXxSVV9cEkNwDvSvKbwGeBi9r4i4A/TjILfBE4axnmLUlagpHhX1XXAk/eT/+tDK7/z+3/MvCCicxOkrQsvMNXkjpk+EtShwx/SeqQ4S9JHTL8JalDhr8kdcjwl6QOGf6S1CHDX5I6ZPhLUocMf0nqkOEvSR0y/CWpQ4a/JHXI8JekDhn+ktQhw1+SOmT4S1KHDH9J6pDhL0kdMvwlqUOGvyR1yPCXpA4Z/pLUIcNfkjpk+EtShwx/SeqQ4S9JHTL8JalDhr8kdcjwl6QOGf6S1KGR4Z/k2CQfS3JDkuuTvLz1H5nkiiQ3t89HtP4keX2S2STXJnnKchchSVqYcc78HwB+vqpOAE4Bzk1yAnAesL2qjge2t2WA04Hj28dW4I0Tn7UkaUlGhn9V3VlVn2ntfwNuBDYAm4Ftbdg24MzW3gy8vQY+CRye5BETn7kkadEWdM0/yQzwZOBK4JiqurOt+jxwTGtvAO4Y2mxn65MkrRJjh3+ShwLvA15RVf86vK6qCqiFHDjJ1iQ7kuzYs2fPQjaVJC3RWOGf5EEMgv9Pq+rPWvcX9l7OaZ93t/5dwLFDm29sffuoqguralNVbVq/fv1i5y9JWoRx3u0T4CLgxqr6g6FVlwFbWnsLcOlQ/0vau35OAe4bujwkSVoF1o0x5hnAi4HPJbm69f0ScD5wSZJzgNuBF7Z1lwNnALPA/cBLJzpjSdKSjQz/qvpbIPOsPm0/4ws4d4nzkiQtI+/wlaQOGf6S1CHDX5I6ZPhLUocMf0nqkOEvSR0y/CWpQ4a/JHXI8JekDhn+ktQhw1+SOmT4S1KHDH9J6pDhL0kdMvwlqUOGvyR1yPCXpA4Z/pLUIcNfkjpk+EtShwx/SeqQ4S9JHTL8JalDhr8kdcjwl6QOGf6S1CHDX5I6ZPhLUocMf0nqkOEvSR0y/CWpQ4a/JHXI8JekDo0M/yRvTbI7yXVDfUcmuSLJze3zEa0/SV6fZDbJtUmespyTlyQtzjhn/hcDz57Tdx6wvaqOB7a3ZYDTgePbx1bgjZOZpiRpkkaGf1V9HPjinO7NwLbW3gacOdT/9hr4JHB4kkdMarKSpMlY7DX/Y6rqztb+PHBMa28A7hgat7P1/T9JtibZkWTHnj17FjkNSdJiLPkF36oqoBax3YVVtamqNq1fv36p05AkLcBiw/8Ley/ntM+7W/8u4NihcRtbnyRpFVls+F8GbGntLcClQ/0vae/6OQW4b+jykCRplVg3akCSdwKnAkcn2Qn8GnA+cEmSc4DbgRe24ZcDZwCzwP3AS5dhzpKkJRoZ/lV19jyrTtvP2ALOXeqkJEnLyzt8JalDhr8kdcjwl6QOGf6S1CHDX5I6ZPhLUocMf0nqkOEvSR0y/CWpQ4a/JHXI8JekDhn+ktQhw1+SOmT4S1KHDH9J6pDhL0kdMvwlqUOGvyR1yPCXpA4Z/pLUIcNfkjpk+EtShwx/SeqQ4S9JHTL8JalDhr8kdcjwl6QOGf6S1CHDX5I6ZPhLUocMf0nqkOEvSR1alvBP8uwkNyWZTXLechxDkrR4Ew//JAcDbwBOB04Azk5ywqSPI0lavOU48z8ZmK2qW6vqv4B3AZuX4TiSpEVatwz73ADcMbS8E3jaMhwHgJnz/mK5dj3Sbec/Z8WOLUlLsRzhP5YkW4GtbfFLSW4aWn00cNfXflYLk9eOPXQq6lkga1r91lo9sAZrymuXVNOjFnvc5Qj/XcCxQ8sbW98+qupC4ML97SDJjqratAxzWxFrrR6wpmmw1uoBa5qk5bjm/2ng+CTHJTkEOAu4bBmOI0lapImf+VfVA0l+CvgQcDDw1qq6ftLHkSQt3rJc86+qy4HLl7CL/V4OmmJrrR6wpmmw1uoBa5qYVNVKHFeStIJ8vIMk9aiqFv0BPBu4CZgFztvP+kOBd7f1VwIzQ+te3fpvAr531D6B49o+Zts+Dxl1jLb+kcCXgF+Y9pqAGeA/gKvbx5umuZ627luBvweuBz4HPHiaawJ+aOj7czXwFeCkKa7nQcC29r25EXj1Gvg5OgR4W6vpGuDUKarpO4HPAA8Az59z/C3Aze1jy8h6xil6ni/EwcAtwKPbF/Ma4IQ5Y36SFlAM3vXz7tY+oY0/tBV5S9vfvPsELgHOau03AT9xoGMMzeG9wHsYI/xXe00Mwv+6tfI9YvCa07XAiW35KODgaa5pzjyeBNwyzfUAPwi8q7UPA25jzgnWFNZ0LvC21n44cBVw0JTUNMPghOntDIU/cCRwa/t8RGsfccCaxg2S/Xwxng58aGj51cw5K2Dwjp+nD/2g3wVk7ti94+bbZ9vmLmDd3GPPd4y2fCbwu8BrGC/8V3VNLDz8V3s9ZwB/stb+3Q3t57eB35rmeoCzgQ+0vqOAfwSOnPKa3gC8eGhf24GTp6GmobEXs2/4nw28eWj5zcDZB6ppKdf89/cYhw3zjamqB4D7GPwDmm/b+fqPAu5t+5h7rP0eI8lDgVcBv75Wamrrjkvy2SR/neQ7pryexwKV5ENJPpPklSPqmYaahr0IeOeU1/Ne4N+BO4F/Bn6vqr445TVdAzwvybokxwHfxr43pq7mmpYyv32s2OMdvgZeA1xQVV9KstJzmZQ7gUdW1d1Jvg348yRPqKp/XemJLdI64NuBpwL3A9uTXFVV21d2WkuX5GnA/VV13UrPZYlOBv4H+CYGlxP+JslHqurWlZ3WkrwVeDywA7gd+DsGNXZlKWf+4zzG4f/GJFkHPAy4+wDbztd/N3B428fcY813jKcBv5PkNuAVwC+1m8+mtqaq+s+quhugqq5icL3wsdNaD4Ozk49X1V1VdT+De0OecoB6pqGmvc5i9Fn/NNTzg8BfVtV/V9Vu4BPAqEcRrOqaquqBqvrZqjqpqjYDhzO4nDUNNS1lfvs60DWhEdfA1jF4UeE4vvpixRPmjDmXfV8AuaS1n8C+L4DcyuDFj3n3yeBF2+EXQH7yQMeYM4/XMN41/1VdE7Ce9oIogxeJdnGA669TUM8RDN65cFjb70eA50zz96gtH9S+N49eA//mXsVXXxx9CHAD8K1TXtNhwENa+1kMTkCm4vs0dKyL+f8v+P4Tg5+pI1r7wK/NjCp6xBfkDAa/MW8Bfrn1/QbwvNZ+cCtiFvgUQz8MwC+37W4CTj/QPofC7lNtX+8BDh11jKFtX8P4b/VctTUBP8DgLZFXMwjN75vmetq6H241XQf8zrR/j9q6U4FProWfI+Chrf96BsH/i2ugppm27xsZnHA8aopqeiqD/zH/O4P/IVw/tM2PtvGzwEtH1eMdvpLUIe/wlaQOGf6S1CHDX5I6ZPhLUocMf0lrVpITk/x9ks8l+UCSb5hn3MuTXJfk+iSvGLV9kkOSvK31X5Pk1KFtXpTk2rav8f/S94HreGuS3UkmdtOg4S9pTUhyapKL53S/hcHTMp8EvB/4xf1s90TgZQzuZj4ReG6Sx4zY/mUArf9ZwO8nOSjJUQyeJ3ZaVT0B+MYkp02gvIsZPAF0Ygx/SWvZY4GPt/YVDO6VmevxwJVVdX8Nnqfz18D3j9j+BOCjADW48/leBnc+Pxq4uar2tHEf2btNkvVJ3pfk0+3jGeMWUVUfB0Y9U2lBDH9Ja9n1wObWfgH7f4DbdcB3JDkqyWEMbrw6dsT28z0cbhZ4XJKZ9niGM4e2eR2D5409lcEvhLdMqMZFWcsPdpPUgSRXMnh0wkOBI5Nc3Va9isFdr69P8qvAZcB/zd2+qm5s1+Y/zODO2av56oPe5tt+vw+Hq6p7kvwEgz/A8pXW/81tm+8GThh60OQ3tKcPP5F5fhFU1RMX9tUYn3f4SloT2ouuP1JVPzLP+scy+PsRJ4/Yz28DO6vqj8bdPsnfAT9WVTfM6d8KPKaqXpnkLmBjVX15AWUN72sG+OCkfiF42UfSmpXk4e3zQcCvMHhI2oHGPZLB9f53HGj7JIcleUhrPwt4YG/wD21zBIO/7rX3rP7DwE8PHfOkCZa6YIa/pLXs7CT/CPwD8C8M/nYvSb4pyeVD496X5AYGf7Xs3Kq690DbM/jzj59JciODy0svHtrX69q+PgGcX1V7Hxf9M8Cm9jbQG4AfH7eIJO9k8LeuH5dkZ5Jzxt123n162UeS+uOZvyR1yPCXpA4Z/pLUIcNfkjpk+EtShwx/SeqQ4S9JHTL8JalD/wtYQ84D8o+8GAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(vals[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5800, device='cuda:7')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.model(data).exp().max(1)[0] > .9999).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9918, 1.0000, 0.5235, 0.9728, 1.0000, 1.0000, 0.9998, 0.9993, 0.9991,\n",
       "        0.9450, 0.9995, 1.0000, 0.9987, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 0.9707, 0.9832, 0.9996, 1.0000, 0.9994, 1.0000, 0.6396,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6656, 1.0000, 0.4900,\n",
       "        0.9662, 0.8447, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999,\n",
       "        1.0000, 0.9998, 0.9969, 1.0000, 0.9969, 1.0000, 1.0000, 0.6264, 0.9999,\n",
       "        1.0000, 1.0000, 0.9999, 0.6083, 0.9196, 1.0000, 1.0000, 0.9286, 1.0000,\n",
       "        0.9436, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9115, 0.6169, 0.9995,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 0.9656, 0.9986, 0.5918, 1.0000, 1.0000,\n",
       "        0.9960, 1.0000, 0.9994, 0.9993, 0.7856, 0.9943, 0.9479, 1.0000, 1.0000,\n",
       "        1.0000, 0.9994, 1.0000, 1.0000, 0.9936, 1.0000, 0.9997, 0.9963, 1.0000,\n",
       "        1.0000], device='cuda:7', grad_fn=<MaxBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model(data).exp().max(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6131, 0.6878, 0.9988, 0.9865, 0.7849, 0.9531, 0.6701, 0.4484, 0.8191,\n",
       "        0.7190, 0.9955, 0.8988, 0.8874, 0.9814, 0.6740, 0.9412, 0.5401, 0.7917,\n",
       "        0.9257, 0.9743], device='cuda:7', grad_fn=<MaxBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model(x).exp().max(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(20, 1, 28, 28, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.9989, 0.9932, 0.9834, 0.9992, 0.9904, 0.9887, 0.9941, 0.9976, 0.9964,\n",
       "        0.9925, 0.9990, 0.9989, 0.9995, 0.9955, 0.9947, 0.9997, 0.9878, 0.9964,\n",
       "        0.9963, 0.9902], device='cuda:7', grad_fn=<MaxBackward0>),\n",
       "indices=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8],\n",
       "       device='cuda:7'))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model(x).exp().max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.0000, 0.9999, 0.9992, 1.0000, 1.0000, 1.0000, 0.9995, 0.9995, 0.9716,\n",
       "        0.9997, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 0.9999, 1.0000, 1.0000,\n",
       "        0.9745, 1.0000, 0.9999, 0.9997, 0.9999, 1.0000, 0.9999, 0.9984, 0.9999,\n",
       "        1.0000, 0.9999, 0.9998, 1.0000, 0.9998, 1.0000, 0.9999, 1.0000, 1.0000,\n",
       "        0.9998, 1.0000, 0.9995, 0.9999, 0.9985, 0.9946, 0.9998, 0.9958, 0.9976,\n",
       "        0.9989, 0.9999, 1.0000, 0.9950, 1.0000, 0.9998, 1.0000, 0.9999, 0.9999,\n",
       "        0.9999, 1.0000, 1.0000, 0.9978, 0.9999, 0.9993, 1.0000, 1.0000, 0.8628,\n",
       "        0.9976, 1.0000, 0.9940, 0.9994, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 0.9864, 0.9999, 0.9998, 1.0000, 0.9999, 0.9924, 0.9996, 0.9994,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9960, 0.9999, 0.9995,\n",
       "        1.0000, 1.0000, 0.9197, 1.0000, 0.9992, 0.9950, 0.9978, 0.9995, 0.9997,\n",
       "        1.0000], device='cuda:7', grad_fn=<MaxBackward0>),\n",
       "indices=tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
       "        4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
       "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4, 6, 4, 3, 0, 7, 0,\n",
       "        2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3, 6, 1, 3, 6, 9, 3, 1, 4,\n",
       "        1, 7, 6, 9], device='cuda:7'))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model(data).exp().max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
