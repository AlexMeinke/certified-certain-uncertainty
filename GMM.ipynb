{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils.models as models\n",
    "import utils.plotting as plotting\n",
    "import utils.dataloaders as dl\n",
    "import utils.traintest as tt\n",
    "import utils.adversarial as adv\n",
    "import utils.eval as ev\n",
    "import model_params as params\n",
    "import utils.resnet_orig as resnet\n",
    "import utils.gmm_helpers as gmm_helpers\n",
    "#import utils.gmm as GMM\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:6')\n",
    "gmm = torch.load('SavedModels/GMM/gmm_MNIST_n1000_data_used60000_augm_flagTrue_alg_scikit.pth').to(device)\n",
    "#gmm = models.GMM(500, 784).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dl.MNIST(train=True, batch_size=512, augm_flag=True)\n",
    "test_loader = dl.MNIST(train=False, augm_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "\n",
    "param_groups = [{'params':gmm.mu,'lr':lr, 'weight_decay':0.},\n",
    "                {'params':gmm.logvar,'lr':lr, 'weight_decay':0.}]\n",
    "optimizer = optim.SGD(param_groups,  momentum=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (data, _) = enumerate(train_loader).__next__()\n",
    "gmm.mu.data = data[:gmm.K].view(-1, 784).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.NLLLoss()\n",
    "gmm.train()\n",
    "\n",
    "train_loss = 0\n",
    "correct = 0\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = torch.logsumexp( gmm(data.view(data.shape[0], -1)), 0 ) - 625.7730167\n",
    "    loss_CEDA = torch.logsumexp(gmm(torch.rand(100, 784, device=device)), 0).mean() - 625.7730167\n",
    "    #loss_CEDA = 0.\n",
    "    loss = -output.mean() + loss_CEDA\n",
    "    loss.backward()\n",
    "    #optimizer.step()\n",
    "\n",
    "    train_loss += loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-3.4188, device='cuda:6', grad_fn=<MeanBackward1>)\n",
      "tensor(0.1372, device='cuda:6', grad_fn=<MeanBackward1>)\n",
      "-151910.87255859375\n",
      "tensor(269.5195, device='cuda:6', grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'abs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-bd3a38ca5748>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'abs'"
     ]
    }
   ],
   "source": [
    "print(gmm.logvar.mean())\n",
    "print(gmm.mu.mean())\n",
    "print(train_loss)\n",
    "print(output.mean())\n",
    "print(gmm.mu.grad.abs().max())\n",
    "print(gmm.logvar.grad.abs().max(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-218.7498, device='cuda:6', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logsumexp(gmm(torch.rand(100, 784, device=device)), 0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-3.3965, -4.8272, -3.4398, -3.2954, -3.6526, -3.5059, -3.7981, -3.3597,\n",
       "        -3.5565, -3.3831, -3.4182, -3.5783, -3.6426, -4.8337, -4.0327, -3.2136,\n",
       "        -4.4995, -4.1311, -3.6154, -3.0753, -3.1343, -3.3480, -3.1363, -3.1603,\n",
       "        -3.0621, -3.2979, -3.1388, -3.1792, -3.2777, -3.5853, -3.5764, -3.7955,\n",
       "        -4.8774, -4.6345, -3.3428, -3.6290, -2.9044, -3.2397, -4.7661, -3.3996,\n",
       "        -3.0726, -3.1795, -4.6050, -3.6613, -3.2569, -3.5025, -3.0670, -3.1325,\n",
       "        -3.1865, -3.4457, -4.6319, -3.5729, -3.4273, -4.2115, -3.4319, -3.0591,\n",
       "        -3.5084, -3.4843, -2.9689, -3.3666, -3.2925, -3.2154, -3.5557, -3.6253,\n",
       "        -3.4599, -3.7843, -3.5400, -3.3434, -3.3730, -3.4570, -2.9498, -3.3055,\n",
       "        -3.5661, -3.2897, -3.4184, -4.7851, -3.2862, -3.3159, -3.1958, -3.2200,\n",
       "        -3.6267, -3.0640, -3.2543, -3.1370, -3.0156, -3.3928, -3.2684, -3.4587,\n",
       "        -3.3792, -3.5496, -3.0836, -3.3368, -3.1415, -3.3738, -3.3695, -3.6379,\n",
       "        -3.3022, -3.2884, -3.4853, -3.7172, -3.2317, -3.4908, -3.7150, -3.4592,\n",
       "        -2.9945, -2.9687, -3.7394, -3.6228, -3.3840, -2.9602, -3.6272, -3.1478,\n",
       "        -3.5207, -4.7477, -3.1788, -3.0938, -3.0313, -3.1070, -3.7511, -3.5036,\n",
       "        -3.4378, -2.8453, -3.4287, -3.2528, -3.0390, -3.4595, -4.5630, -3.4046,\n",
       "        -3.8975, -3.2643, -4.1891, -3.7746, -3.5143, -3.3204, -3.3261, -4.6122,\n",
       "        -3.5265, -3.2756, -3.5250, -3.3859, -3.5987, -3.8094, -3.0699, -3.3291,\n",
       "        -3.5601, -3.3181, -4.5992, -3.1609, -3.1982, -3.1224, -3.3633, -3.2223,\n",
       "        -3.0886, -3.3745, -3.3058, -3.7589, -3.4772, -3.3027, -3.3885, -3.5789,\n",
       "        -3.4070, -3.1653, -3.6892, -3.3957, -3.2728, -3.3410, -4.4551, -3.4410,\n",
       "        -3.3689, -4.7046, -3.3204, -3.3833, -3.7686, -3.7178, -3.4518, -3.6816,\n",
       "        -3.4727, -3.2954, -3.0885, -3.2851, -3.5652, -3.2272, -3.5254, -3.6777,\n",
       "        -3.4136, -3.5095, -3.0808, -4.4817, -3.3485, -3.2760, -3.2695, -3.0926,\n",
       "        -3.2462, -3.4578, -3.0803, -3.1331, -3.6623, -3.3227, -3.1012, -3.7903,\n",
       "        -3.1091, -4.4291, -3.1863, -3.8511, -3.5482, -3.0060, -3.2572, -3.4049,\n",
       "        -3.4962, -4.6966, -3.5315, -3.2270, -3.3327, -3.4205, -3.3038, -2.9211,\n",
       "        -3.3567, -4.4471, -3.6135, -3.4244, -4.3933, -3.3306, -4.7222, -3.2254,\n",
       "        -3.6256, -3.1857, -3.1529, -3.7972, -3.3511, -3.4214, -3.2330, -3.6422,\n",
       "        -3.3599, -3.2497, -3.0205, -3.3671, -3.4478, -3.3155, -3.1943, -3.1373,\n",
       "        -4.0526, -3.2829, -3.3026, -4.6505, -3.1630, -3.5882, -3.1809, -3.6121,\n",
       "        -3.3117, -2.9142, -3.1569, -3.1823, -3.4380, -3.3539, -3.3053, -3.3326,\n",
       "        -3.3805, -3.2052, -3.1184, -3.6353, -4.2722, -3.2620, -3.2648, -3.5386,\n",
       "        -3.3076, -3.2230, -3.1680, -3.3370, -3.4225, -3.2445, -3.0434, -3.3154,\n",
       "        -3.3948, -3.4593, -3.4991, -3.1014, -2.9755, -3.3291, -3.4598, -3.6009,\n",
       "        -3.2993, -3.2099, -3.2078, -3.3069, -3.5257, -3.2789, -3.6327, -3.6684,\n",
       "        -3.2978, -3.3961, -3.1577, -3.1880, -3.1589, -3.2657, -4.5500, -3.6212,\n",
       "        -4.6010, -3.5041, -3.3155, -3.4041, -3.1048, -3.3661, -3.4913, -3.2148,\n",
       "        -3.1835, -3.4465, -3.7849, -3.3470, -3.5803, -3.5187, -3.2562, -3.3321,\n",
       "        -3.1700, -2.9720, -3.2310, -3.6712, -3.0865, -2.9641, -3.3735, -4.6689,\n",
       "        -3.1331, -3.1793, -3.1312, -3.0743, -3.4895, -3.4772, -3.4323, -3.5677,\n",
       "        -3.1007, -3.3628, -4.6861, -3.2834, -3.5516, -3.1305, -3.1074, -3.2327,\n",
       "        -3.0419, -3.4506, -3.3303, -3.0565, -4.6008, -3.0689, -3.3328, -2.8985,\n",
       "        -3.7630, -3.1830, -3.1614, -3.5788, -3.4023, -3.3260, -3.5598, -3.1294,\n",
       "        -3.4665, -3.1560, -3.0617, -3.4505, -3.2076, -3.0071, -3.2254, -2.9978,\n",
       "        -3.6460, -3.4284, -3.2776, -3.5379, -3.3395, -3.4741, -3.2884, -3.3986,\n",
       "        -3.3556, -3.6790, -3.3947, -3.7470, -3.1312, -3.4069, -3.2173, -3.4631,\n",
       "        -3.4348, -3.3952, -2.9301, -3.5024, -3.3575, -3.1767, -3.3577, -2.9248,\n",
       "        -3.5386, -3.2818, -3.2689, -3.7093, -3.7012, -2.9160, -3.2450, -3.4847,\n",
       "        -3.3578, -2.9345, -3.4835, -3.2645, -4.6827, -3.3860, -3.1421, -3.5329,\n",
       "        -3.3223, -3.5871, -4.5128, -3.2574, -3.1426, -3.3344, -3.2858, -3.2807,\n",
       "        -3.2294, -3.4256, -3.1135, -2.9132, -3.1732, -3.7344, -3.4405, -4.2599,\n",
       "        -3.3548, -3.1606, -3.3078, -2.8493, -3.2880, -3.4923, -3.5744, -3.5364,\n",
       "        -3.2739, -3.4944, -3.4191, -3.5295, -3.6146, -3.3229, -3.2889, -3.7148,\n",
       "        -3.6376, -3.1079, -3.4820, -4.6991, -3.4634, -3.8380, -3.3510, -3.2814,\n",
       "        -3.5363, -3.4770, -3.1580, -3.2731, -3.0471, -3.4030, -3.1826, -3.2833,\n",
       "        -3.2888, -3.1617, -3.6796, -3.6649, -3.5055, -3.3990, -3.2366, -3.4690,\n",
       "        -3.2135, -3.1635, -3.3618, -3.5743, -3.2378, -3.5191, -2.9751, -3.8702,\n",
       "        -3.4243, -3.1612, -3.0155, -3.2547, -3.3926, -4.6482, -2.9720, -3.4978,\n",
       "        -3.4662, -3.2309, -3.3299, -3.3319, -3.7195, -3.4597, -3.1119, -3.2086,\n",
       "        -3.0959, -3.4170, -2.8889, -3.5459, -2.9040, -3.2518, -3.1738, -3.3389,\n",
       "        -3.4515, -3.1772, -3.5726, -3.0637, -3.2839, -3.2774, -4.6775, -3.9163,\n",
       "        -3.4818, -2.9903, -4.7123, -3.2841, -3.3314, -3.8702, -3.4240, -3.0888,\n",
       "        -3.3491, -4.2920, -3.0667, -2.9653, -3.7204, -3.3788, -3.3290, -3.4272,\n",
       "        -3.3876, -3.0757, -3.4788, -3.9361, -3.3940, -3.1084, -3.4229, -3.2640,\n",
       "        -3.5247, -4.2850, -3.1666, -3.2139, -3.4410, -3.2093, -3.3610, -3.8022,\n",
       "        -3.1134, -3.2656, -3.5443, -3.2526, -4.6198, -3.4600, -3.0552, -3.1213,\n",
       "        -3.1690, -3.2608, -3.0281, -3.3370, -3.2698, -3.1072, -3.3830, -3.5514,\n",
       "        -3.5907, -4.7265, -3.1375, -3.5398, -3.5986, -3.3288, -3.6646, -3.0506,\n",
       "        -3.1488, -3.3046, -3.4383, -4.4158, -3.2633, -4.7857, -3.1401, -3.2816,\n",
       "        -2.9714, -3.1013, -3.0249, -3.1325, -3.5858, -3.5660, -3.3336, -3.6650,\n",
       "        -3.1157, -3.5324, -3.3832, -3.3546, -3.3178, -3.3977, -3.0375, -3.1203,\n",
       "        -3.3993, -3.6026, -4.3137, -4.4668, -3.1221, -3.1402, -3.3651, -3.3202,\n",
       "        -3.0278, -3.0899, -3.0521, -3.4857, -3.4620, -3.1686, -4.3634, -3.4343,\n",
       "        -3.2808, -4.5547, -3.5466, -3.2056, -2.9867, -3.2297, -3.2851, -4.1418,\n",
       "        -3.3630, -3.5360, -3.1649, -3.4733, -2.9892, -3.1585, -3.1023, -3.5541,\n",
       "        -3.2629, -3.2934, -3.3896, -3.3518, -3.4254, -3.1128, -3.2097, -3.2895,\n",
       "        -3.5688, -3.5098, -3.0514, -3.1520, -2.8504, -3.7502, -3.5474, -3.5802,\n",
       "        -3.5390, -3.3714, -3.0749, -3.3078, -3.2794, -3.3510, -3.2264, -3.0685,\n",
       "        -2.8017, -3.3330, -3.3965, -2.9652, -3.2941, -3.6845, -3.5488, -3.3736,\n",
       "        -3.0579, -3.4059, -3.2801, -3.1280, -4.6146, -3.1434, -3.6272, -3.4482,\n",
       "        -3.3097, -3.3855, -3.2692, -3.6986, -2.9637, -3.5761, -2.8822, -3.2263,\n",
       "        -3.6063, -3.1309, -3.6838, -3.4361, -2.9698, -3.1348, -3.2394, -3.3496,\n",
       "        -4.5840, -3.2611, -3.3583, -3.5610, -3.1726, -3.5228, -3.1114, -3.0573,\n",
       "        -3.5636, -3.3590, -3.1471, -3.5632, -3.0591, -3.0893, -3.2684, -3.1898,\n",
       "        -3.3321, -3.0960, -3.2079, -3.5879, -3.5883, -3.4399, -3.0671, -3.0001,\n",
       "        -3.2188, -3.0782, -3.2692, -3.3404, -3.5835, -3.2725, -3.3490, -3.8635,\n",
       "        -2.9543, -3.4965, -3.1732, -4.4689, -3.7234, -3.2771, -2.8880, -3.5006,\n",
       "        -3.2922, -3.4265, -3.8907, -3.1878, -3.4245, -3.0704, -3.8623, -3.3226,\n",
       "        -3.0911, -3.5203, -3.6142, -3.1163, -3.2317, -3.0401, -3.0587, -3.0399,\n",
       "        -3.4002, -3.6124, -3.2213, -3.3664, -3.2147, -3.1688, -3.2411, -3.3204,\n",
       "        -4.6422, -3.4292, -3.2574, -3.0215, -3.1992, -3.4322, -3.2939, -2.9733,\n",
       "        -3.4122, -3.0426, -3.4791, -3.9654, -3.0136, -4.7878, -3.3002, -3.2936,\n",
       "        -3.6740, -3.0634, -3.3663, -3.1414, -3.7393, -3.4262, -3.1392, -3.2453,\n",
       "        -4.5594, -3.1520, -3.2213, -3.3009, -3.3461, -3.3065, -3.3240, -3.3613,\n",
       "        -3.4531, -3.8464, -2.9866, -3.1761, -3.0306, -3.3744, -3.3622, -3.0036,\n",
       "        -3.3074, -3.2091, -3.3015, -3.1137, -3.2030, -3.2204, -3.3030, -3.0255,\n",
       "        -3.2432, -3.6907, -3.3722, -3.7155, -3.1345, -3.4807, -3.7595, -3.3218,\n",
       "        -3.7073, -3.4230, -3.7074, -4.6962, -3.3189, -2.7004, -2.9317, -3.3402,\n",
       "        -3.3878, -3.2375, -3.4385, -3.4516, -3.1390, -3.1843, -4.0460, -2.9979,\n",
       "        -3.1766, -3.0270, -3.6487, -3.5997, -3.3752, -4.6147, -3.2519, -3.0556,\n",
       "        -3.3979, -3.1556, -3.0434, -4.2571, -3.1470, -3.6544, -4.4446, -3.2499,\n",
       "        -3.1023, -2.9807, -3.3017, -3.5765, -3.5724, -4.5264, -3.0894, -3.6949,\n",
       "        -3.2068, -3.1644, -4.4929, -3.4678, -3.3221, -3.3359, -3.1193, -3.2036,\n",
       "        -3.6363, -3.7819, -3.8300, -3.3235, -3.3250, -3.1000, -3.0857, -2.9705,\n",
       "        -3.3591, -2.9167, -3.6210, -3.2083, -3.0873, -3.7597, -3.4239, -2.9647,\n",
       "        -3.3747, -3.0409, -3.6595, -3.0656, -3.2066, -3.5113, -4.3708, -4.6103,\n",
       "        -2.9389, -3.0011, -2.9549, -3.3745, -3.2814, -2.6780, -4.4475, -3.3493,\n",
       "        -3.3797, -3.3023, -3.4622, -3.4419, -3.3565, -4.4247, -4.4139, -3.4360,\n",
       "        -3.5674, -3.8408, -3.3471, -3.4955, -3.3151, -3.0398, -3.4255, -3.1295,\n",
       "        -3.4552, -4.1840, -4.7557, -3.3728, -4.5373, -3.2869, -3.4214, -3.2226,\n",
       "        -3.6524, -4.4764, -3.4623, -3.3221, -3.0061, -3.1762, -3.1439, -3.3324,\n",
       "        -3.4590, -4.5786, -3.6271, -3.4792, -3.2190, -3.4532, -2.9742, -3.4945,\n",
       "        -3.1798, -3.1828, -3.3516, -3.4805, -3.2898, -3.2195, -2.9040, -3.0346,\n",
       "        -3.4712, -3.2208, -3.1739, -3.0455, -3.2041, -3.1788, -3.1171, -3.1238,\n",
       "        -3.0257, -3.3974, -3.1711, -3.2148, -3.4349, -3.1895, -3.5632, -3.5328,\n",
       "        -4.8068, -3.0589, -3.3631, -3.1704, -3.0667, -3.3343, -3.6188, -3.6368,\n",
       "        -3.5185, -3.3164, -3.1945, -3.5019, -3.4005, -3.5903, -3.2533, -2.9791,\n",
       "        -3.6573, -3.0225, -3.2124, -3.2851, -3.1561, -3.0784, -3.2588, -4.6004,\n",
       "        -3.3652, -3.4241, -3.3279, -3.6827, -3.2837, -3.3095, -3.3815, -3.2137,\n",
       "        -3.2115, -3.3998, -3.3101, -3.0605, -3.1087, -3.3335, -3.0755, -3.2625,\n",
       "        -3.1762, -3.0448, -3.5661, -4.7993, -3.2710, -3.5057, -3.9814, -3.0498,\n",
       "        -3.6443, -3.2867, -3.3876, -3.4958, -3.1515, -3.2745, -3.4486, -3.0924,\n",
       "        -3.3241, -3.6521, -3.2266, -2.8467, -3.3922, -3.5998, -3.7376, -3.1119,\n",
       "        -3.4611, -3.2289, -3.1896, -3.6812, -3.0741, -2.9917, -3.4301, -3.2305],\n",
       "       device='cuda:6', requires_grad=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1830, 0.0895, 0.1791, 0.1925, 0.1610, 0.1733, 0.1497, 0.1864, 0.1689,\n",
       "        0.1842, 0.1810, 0.1671, 0.1618, 0.0892, 0.1331, 0.2005, 0.1054, 0.1267,\n",
       "        0.1640, 0.2149, 0.2086, 0.1875, 0.2084, 0.2059, 0.2163, 0.1923, 0.2082,\n",
       "        0.2040, 0.1942, 0.1665, 0.1673, 0.1499, 0.0873, 0.0985, 0.1880, 0.1629,\n",
       "        0.2341, 0.1979, 0.0923, 0.1827, 0.2152, 0.2040, 0.1000, 0.1603, 0.1962,\n",
       "        0.1736, 0.2158, 0.2088, 0.2033, 0.1786, 0.0987, 0.1676, 0.1802, 0.1218,\n",
       "        0.1798, 0.2166, 0.1730, 0.1751, 0.2266, 0.1858, 0.1928, 0.2003, 0.1690,\n",
       "        0.1632, 0.1773, 0.1507, 0.1703, 0.1879, 0.1852, 0.1775, 0.2288, 0.1915,\n",
       "        0.1681, 0.1930, 0.1810, 0.0914, 0.1934, 0.1905, 0.2023, 0.1999, 0.1631,\n",
       "        0.2161, 0.1965, 0.2084, 0.2214, 0.1833, 0.1951, 0.1774, 0.1846, 0.1695,\n",
       "        0.2140, 0.1885, 0.2079, 0.1851, 0.1855, 0.1622, 0.1918, 0.1932, 0.1751,\n",
       "        0.1559, 0.1987, 0.1746, 0.1561, 0.1774, 0.2237, 0.2266, 0.1542, 0.1634,\n",
       "        0.1842, 0.2276, 0.1631, 0.2072, 0.1720, 0.0931, 0.2040, 0.2129, 0.2197,\n",
       "        0.2115, 0.1533, 0.1735, 0.1793, 0.2411, 0.1801, 0.1966, 0.2188, 0.1773,\n",
       "        0.1021, 0.1823, 0.1425, 0.1955, 0.1231, 0.1515, 0.1725, 0.1901, 0.1896,\n",
       "        0.0997, 0.1715, 0.1944, 0.1716, 0.1840, 0.1654, 0.1489, 0.2155, 0.1893,\n",
       "        0.1686, 0.1903, 0.1003, 0.2059, 0.2021, 0.2099, 0.1861, 0.1997, 0.2135,\n",
       "        0.1850, 0.1915, 0.1527, 0.1758, 0.1918, 0.1837, 0.1671, 0.1820, 0.2054,\n",
       "        0.1581, 0.1831, 0.1947, 0.1882, 0.1078, 0.1790, 0.1855, 0.0951, 0.1901,\n",
       "        0.1842, 0.1519, 0.1558, 0.1780, 0.1587, 0.1762, 0.1925, 0.2135, 0.1935,\n",
       "        0.1682, 0.1992, 0.1716, 0.1590, 0.1814, 0.1730, 0.2143, 0.1064, 0.1875,\n",
       "        0.1944, 0.1950, 0.2130, 0.1973, 0.1775, 0.2144, 0.2088, 0.1602, 0.1899,\n",
       "        0.2121, 0.1503, 0.2113, 0.1092, 0.2033, 0.1458, 0.1696, 0.2225, 0.1962,\n",
       "        0.1822, 0.1741, 0.0955, 0.1711, 0.1992, 0.1889, 0.1808, 0.1917, 0.2321,\n",
       "        0.1867, 0.1082, 0.1642, 0.1805, 0.1112, 0.1891, 0.0943, 0.1993, 0.1632,\n",
       "        0.2033, 0.2067, 0.1498, 0.1872, 0.1807, 0.1986, 0.1618, 0.1864, 0.1969,\n",
       "        0.2209, 0.1857, 0.1784, 0.1906, 0.2025, 0.2083, 0.1318, 0.1937, 0.1918,\n",
       "        0.0978, 0.2057, 0.1663, 0.2038, 0.1643, 0.1909, 0.2329, 0.2063, 0.2037,\n",
       "        0.1792, 0.1869, 0.1915, 0.1889, 0.1845, 0.2014, 0.2103, 0.1624, 0.1181,\n",
       "        0.1957, 0.1955, 0.1705, 0.1913, 0.1996, 0.2052, 0.1885, 0.1806, 0.1975,\n",
       "        0.2183, 0.1906, 0.1832, 0.1773, 0.1739, 0.2121, 0.2259, 0.1893, 0.1773,\n",
       "        0.1652, 0.1921, 0.2009, 0.2011, 0.1914, 0.1716, 0.1941, 0.1626, 0.1597,\n",
       "        0.1923, 0.1830, 0.2062, 0.2031, 0.2061, 0.1954, 0.1028, 0.1636, 0.1002,\n",
       "        0.1734, 0.1906, 0.1823, 0.2117, 0.1858, 0.1745, 0.2004, 0.2036, 0.1785,\n",
       "        0.1507, 0.1876, 0.1669, 0.1722, 0.1963, 0.1890, 0.2049, 0.2263, 0.1988,\n",
       "        0.1595, 0.2137, 0.2272, 0.1851, 0.0969, 0.2088, 0.2040, 0.2090, 0.2150,\n",
       "        0.1747, 0.1758, 0.1798, 0.1680, 0.2122, 0.1861, 0.0960, 0.1937, 0.1694,\n",
       "        0.2090, 0.2115, 0.1986, 0.2185, 0.1781, 0.1892, 0.2169, 0.1002, 0.2156,\n",
       "        0.1889, 0.2347, 0.1524, 0.2036, 0.2058, 0.1671, 0.1825, 0.1896, 0.1687,\n",
       "        0.2092, 0.1767, 0.2064, 0.2164, 0.1781, 0.2011, 0.2223, 0.1993, 0.2234,\n",
       "        0.1615, 0.1801, 0.1942, 0.1705, 0.1883, 0.1760, 0.1932, 0.1828, 0.1868,\n",
       "        0.1589, 0.1832, 0.1536, 0.2090, 0.1821, 0.2002, 0.1770, 0.1795, 0.1831,\n",
       "        0.2311, 0.1736, 0.1866, 0.2043, 0.1866, 0.2317, 0.1705, 0.1938, 0.1951,\n",
       "        0.1565, 0.1571, 0.2327, 0.1974, 0.1751, 0.1866, 0.2306, 0.1752, 0.1955,\n",
       "        0.0962, 0.1840, 0.2078, 0.1709, 0.1899, 0.1664, 0.1047, 0.1962, 0.2078,\n",
       "        0.1888, 0.1934, 0.1939, 0.1989, 0.1804, 0.2108, 0.2330, 0.2046, 0.1546,\n",
       "        0.1790, 0.1188, 0.1869, 0.2059, 0.1913, 0.2406, 0.1932, 0.1744, 0.1674,\n",
       "        0.1706, 0.1946, 0.1743, 0.1809, 0.1712, 0.1641, 0.1899, 0.1931, 0.1561,\n",
       "        0.1622, 0.2114, 0.1753, 0.0954, 0.1770, 0.1468, 0.1872, 0.1938, 0.1707,\n",
       "        0.1758, 0.2062, 0.1946, 0.2179, 0.1824, 0.2037, 0.1937, 0.1931, 0.2058,\n",
       "        0.1588, 0.1600, 0.1733, 0.1828, 0.1982, 0.1765, 0.2005, 0.2056, 0.1862,\n",
       "        0.1674, 0.1981, 0.1721, 0.2259, 0.1444, 0.1805, 0.2059, 0.2214, 0.1965,\n",
       "        0.1834, 0.0979, 0.2263, 0.1740, 0.1767, 0.1988, 0.1892, 0.1890, 0.1557,\n",
       "        0.1773, 0.2110, 0.2010, 0.2127, 0.1811, 0.2359, 0.1698, 0.2341, 0.1967,\n",
       "        0.2046, 0.1884, 0.1780, 0.2042, 0.1676, 0.2161, 0.1936, 0.1942, 0.0964,\n",
       "        0.1411, 0.1754, 0.2242, 0.0948, 0.1936, 0.1891, 0.1444, 0.1805, 0.2134,\n",
       "        0.1874, 0.1169, 0.2158, 0.2270, 0.1556, 0.1846, 0.1893, 0.1802, 0.1838,\n",
       "        0.2148, 0.1756, 0.1397, 0.1832, 0.2114, 0.1806, 0.1955, 0.1716, 0.1174,\n",
       "        0.2053, 0.2005, 0.1790, 0.2010, 0.1863, 0.1494, 0.2108, 0.1954, 0.1700,\n",
       "        0.1967, 0.0993, 0.1773, 0.2171, 0.2100, 0.2051, 0.1959, 0.2200, 0.1885,\n",
       "        0.1950, 0.2115, 0.1842, 0.1694, 0.1661, 0.0941, 0.2083, 0.1704, 0.1654,\n",
       "        0.1893, 0.1600, 0.2176, 0.2071, 0.1916, 0.1792, 0.1099, 0.1956, 0.0914,\n",
       "        0.2080, 0.1938, 0.2263, 0.2121, 0.2204, 0.2088, 0.1665, 0.1681, 0.1888,\n",
       "        0.1600, 0.2106, 0.1710, 0.1842, 0.1869, 0.1903, 0.1829, 0.2190, 0.2101,\n",
       "        0.1828, 0.1651, 0.1157, 0.1072, 0.2099, 0.2080, 0.1859, 0.1901, 0.2201,\n",
       "        0.2133, 0.2174, 0.1750, 0.1771, 0.2051, 0.1129, 0.1796, 0.1939, 0.1026,\n",
       "        0.1698, 0.2013, 0.2246, 0.1989, 0.1935, 0.1261, 0.1861, 0.1707, 0.2055,\n",
       "        0.1761, 0.2243, 0.2061, 0.2120, 0.1691, 0.1957, 0.1927, 0.1836, 0.1871,\n",
       "        0.1804, 0.2109, 0.2009, 0.1931, 0.1679, 0.1729, 0.2175, 0.2068, 0.2405,\n",
       "        0.1533, 0.1697, 0.1669, 0.1704, 0.1853, 0.2149, 0.1913, 0.1940, 0.1872,\n",
       "        0.1992, 0.2156, 0.2464, 0.1889, 0.1830, 0.2270, 0.1926, 0.1585, 0.1696,\n",
       "        0.1851, 0.2168, 0.1821, 0.1940, 0.2093, 0.0995, 0.2077, 0.1631, 0.1783,\n",
       "        0.1911, 0.1840, 0.1950, 0.1573, 0.2272, 0.1673, 0.2367, 0.1993, 0.1648,\n",
       "        0.2090, 0.1585, 0.1794, 0.2265, 0.2086, 0.1980, 0.1873, 0.1011, 0.1958,\n",
       "        0.1865, 0.1686, 0.2047, 0.1718, 0.2110, 0.2168, 0.1683, 0.1865, 0.2073,\n",
       "        0.1684, 0.2166, 0.2134, 0.1951, 0.2029, 0.1890, 0.2127, 0.2011, 0.1663,\n",
       "        0.1663, 0.1791, 0.2158, 0.2231, 0.2000, 0.2146, 0.1950, 0.1882, 0.1667,\n",
       "        0.1947, 0.1874, 0.1449, 0.2283, 0.1741, 0.2046, 0.1070, 0.1554, 0.1943,\n",
       "        0.2360, 0.1737, 0.1928, 0.1803, 0.1429, 0.2031, 0.1805, 0.2154, 0.1450,\n",
       "        0.1899, 0.2132, 0.1720, 0.1641, 0.2105, 0.1987, 0.2187, 0.2167, 0.2187,\n",
       "        0.1827, 0.1643, 0.1998, 0.1858, 0.2004, 0.2051, 0.1978, 0.1901, 0.0982,\n",
       "        0.1800, 0.1962, 0.2207, 0.2020, 0.1798, 0.1926, 0.2261, 0.1816, 0.2184,\n",
       "        0.1756, 0.1377, 0.2216, 0.0913, 0.1920, 0.1927, 0.1593, 0.2162, 0.1858,\n",
       "        0.2079, 0.1542, 0.1803, 0.2081, 0.1974, 0.1023, 0.2068, 0.1998, 0.1920,\n",
       "        0.1877, 0.1914, 0.1898, 0.1863, 0.1779, 0.1461, 0.2246, 0.2043, 0.2197,\n",
       "        0.1850, 0.1862, 0.2227, 0.1913, 0.2010, 0.1919, 0.2108, 0.2016, 0.1998,\n",
       "        0.1918, 0.2203, 0.1976, 0.1580, 0.1852, 0.1560, 0.2086, 0.1755, 0.1526,\n",
       "        0.1900, 0.1567, 0.1806, 0.1567, 0.0956, 0.1902, 0.2592, 0.2309, 0.1882,\n",
       "        0.1838, 0.1981, 0.1792, 0.1780, 0.2081, 0.2035, 0.1323, 0.2234, 0.2043,\n",
       "        0.2201, 0.1613, 0.1653, 0.1850, 0.0995, 0.1967, 0.2170, 0.1829, 0.2064,\n",
       "        0.2183, 0.1190, 0.2073, 0.1609, 0.1084, 0.1969, 0.2120, 0.2253, 0.1919,\n",
       "        0.1672, 0.1676, 0.1040, 0.2134, 0.1576, 0.2012, 0.2055, 0.1058, 0.1766,\n",
       "        0.1899, 0.1886, 0.2102, 0.2015, 0.1623, 0.1509, 0.1473, 0.1898, 0.1897,\n",
       "        0.2122, 0.2138, 0.2264, 0.1865, 0.2326, 0.1636, 0.2011, 0.2136, 0.1526,\n",
       "        0.1805, 0.2271, 0.1850, 0.2186, 0.1605, 0.2159, 0.2012, 0.1728, 0.1124,\n",
       "        0.0997, 0.2300, 0.2230, 0.2282, 0.1850, 0.1938, 0.2621, 0.1082, 0.1874,\n",
       "        0.1845, 0.1918, 0.1771, 0.1789, 0.1867, 0.1094, 0.1100, 0.1794, 0.1680,\n",
       "        0.1465, 0.1876, 0.1742, 0.1906, 0.2187, 0.1804, 0.2091, 0.1777, 0.1234,\n",
       "        0.0927, 0.1852, 0.1035, 0.1933, 0.1807, 0.1996, 0.1610, 0.1066, 0.1771,\n",
       "        0.1899, 0.2224, 0.2043, 0.2076, 0.1890, 0.1774, 0.1013, 0.1631, 0.1756,\n",
       "        0.2000, 0.1779, 0.2260, 0.1743, 0.2039, 0.2036, 0.1872, 0.1755, 0.1930,\n",
       "        0.1999, 0.2341, 0.2193, 0.1763, 0.1998, 0.2045, 0.2181, 0.2015, 0.2040,\n",
       "        0.2104, 0.2097, 0.2203, 0.1829, 0.2048, 0.2004, 0.1795, 0.2030, 0.1684,\n",
       "        0.1709, 0.0904, 0.2167, 0.1861, 0.2049, 0.2158, 0.1888, 0.1638, 0.1623,\n",
       "        0.1722, 0.1905, 0.2025, 0.1736, 0.1826, 0.1661, 0.1966, 0.2255, 0.1606,\n",
       "        0.2206, 0.2007, 0.1935, 0.2064, 0.2146, 0.1960, 0.1002, 0.1859, 0.1805,\n",
       "        0.1894, 0.1586, 0.1936, 0.1911, 0.1844, 0.2005, 0.2007, 0.1827, 0.1911,\n",
       "        0.2165, 0.2113, 0.1889, 0.2149, 0.1957, 0.2043, 0.2182, 0.1681, 0.0907,\n",
       "        0.1949, 0.1733, 0.1366, 0.2176, 0.1617, 0.1933, 0.1838, 0.1741, 0.2069,\n",
       "        0.1945, 0.1783, 0.2131, 0.1898, 0.1610, 0.1992, 0.2409, 0.1834, 0.1653,\n",
       "        0.1543, 0.2110, 0.1772, 0.1990, 0.2030, 0.1587, 0.2150, 0.2241, 0.1800,\n",
       "        0.1988], device='cuda:6', grad_fn=<SqrtBackward>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm.logvar.exp().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, min_conf=.1):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0.\n",
    "    av_conf = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            c, pred = output.max(1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += (pred.eq(target.view_as(pred))*(c.exp()>min_conf)).sum().item()\n",
    "            av_conf += c.exp().sum().item()\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    av_conf /= len(test_loader.dataset)\n",
    "    correct /= len(test_loader.dataset)\n",
    "    \n",
    "    return correct, av_conf, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(924.5433, device='cuda:6', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.models' from '/home/alexm/project/notebooks/gmm-robust/utils/models.py'>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
